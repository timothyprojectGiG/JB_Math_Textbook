{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62ed965",
   "metadata": {},
   "source": [
    "# Solution to Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566dd239",
   "metadata": {},
   "source": [
    "```{index} normal equations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a63d81",
   "metadata": {},
   "source": [
    "## OLS Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69725d2",
   "metadata": {},
   "source": [
    ":::{toggle}\n",
    "\n",
    "Problem 1) \n",
    "The system is\n",
    "\n",
    "\\begin{align*}\n",
    "a+b&=0,\\\\\n",
    "a + 4b&=5,\\\\\n",
    "a+7b&=8;\n",
    "\\end{align*}\n",
    "\n",
    "or, in matrix form,\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "1& 1\\\\\n",
    "1& 4\\\\\n",
    "1&7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "5\\\\\n",
    "8\\\\\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "The normal equations are\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "1& 1&1\\\\\n",
    "1&4&7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0\\\\\n",
    "5\\\\\n",
    "8\\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "1& 1&1\\\\\n",
    "1&4&7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1& 1\\\\\n",
    "1& 4\\\\\n",
    "1&7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\n",
    "\\end{pmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "or\n",
    "\t\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "13\\\\\n",
    "76\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "3&12\\\\\n",
    "12&66\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Solving the normal equations gives the least-squares solution $a=-1$,  $b=4/3$. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddfc67",
   "metadata": {},
   "source": [
    ":::{toggle}\n",
    "\n",
    "\n",
    "Problem 2)\n",
    "\n",
    "The system is\n",
    "\t\n",
    "\\begin{align*}\n",
    "a-b+c&=1,\\\\\n",
    "a&=0,\\\\\n",
    "a+b+c&=2,\\\\\n",
    "a+2b+4c&=3;\\\\\n",
    "\\end{align*}\n",
    "\t\n",
    "or, in matrix form,\n",
    "\t\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "1& -1&1\\\\\n",
    "1& 0&0\\\\\n",
    "1&1&1\\\\\n",
    "1& 2&4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "The normal equations are\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "1& 1&1&1\\\\\n",
    "-1&0&1&2\\\\\n",
    "1&0&1&4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "1& 1&1&1\\\\\n",
    "-1&0&1&2\\\\\n",
    "1&0&1&4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1& -1&1\\\\\n",
    "1& 0&0\\\\\n",
    "1&1&1\\\\\n",
    "1& 2&4\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\n",
    "\\end{pmatrix},\n",
    "\\end{align*}\n",
    "or\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "6\\\\\n",
    "7\\\\\n",
    "15\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "4&2&6\\\\\n",
    "2&6&8\\\\\n",
    "6&8&18\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Solving the normal equations gives the least-squares solution $a=3/5, b=3/10$, and $c=1/2.$\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de94050",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d3be7",
   "metadata": {},
   "source": [
    "```{ k-means clustering\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695fd4f",
   "metadata": {},
   "source": [
    "Problem 1) $\\mathbf{w}_1=(-4,-1.2)$,  $\\mathbf{w}_2=(-3.2,-0.72)$ and  $\\mathbf{w}_3=(-2.56,-0.432)$. The gradient descent sequence will converge to the minimum point at $(0,0)$. To prove this, note that if we let $\\mathbf{w}_n=(x_n,y_n)$, then $\\mathbf{w}_n=\\mathbf{w}_{n-1}-0.1\\nabla \\mathbf{w}_{n-1}$ implies that\n",
    "$x_n=0.8x_{n-1}$ and $y_n=0.6y_{n-1}$. It follows that $\\lim_{n\\rightarrow \\infty} x_n=\\lim_{n\\rightarrow \\infty} y_n=0.$\n",
    "\n",
    "Problem 2) \n",
    "\n",
    "i)Fix the prototypes $z_1=(-1,0)$ and $z_2=(1,0)$. Then we have $y_{1,1}=1$, $y_{1,2}=0$, $y_{2,1}=1$, $y_{2,2}=0$, $y_{3,1}=1$, $y_{3,2}=0$,\n",
    "    $y_{4,1}=0$, $y_{4,2}=1$, $y_{5,1}=0$, $y_{5,2}=1$, $y_{6,1}=0$, $y_{6,2}=1$.\n",
    "    \n",
    "ii) Fix each of these assignments, then the prototypes are unchanged: $\\mathbf{z}_1=(-1,0)$, $\\mathbf{z}_2=(1,0)$.\n",
    "\n",
    "iii) Fix the prototypes  and the assignments are as before.\n",
    "\n",
    "\n",
    "Thus, the points  $(-1,-1)$, $(-1,0)$, $(-1,1)$ are assigned to cluster 1, and the points $(1,-1)$, $(1,0)$, and $(1,1)$ to cluster 2.\n",
    "\n",
    "\n",
    "Problem 3)\n",
    "\n",
    "i) Fix the prototypes $z_1=(-1,0)$ and $z_2=(1,0)$. Then we have $y_{1,1}=1$, $y_{1,2}=0$, $y_{2,1}=1$, $y_{2,2}=0$, $y_{3,1}=1$, $y_{3,2}=0$,\n",
    "    $y_{4,1}=0$, $y_{4,2}=1$, $y_{5,1}=0$, $y_{5,2}=1$, $y_{6,1}=0$, $y_{6,2}=1$.\n",
    "    \n",
    "ii) Fix each of these assignments, then the prototypes are unchanged: $\\mathbf{z}_1=(-1,0)$, $\\mathbf{z}_2=(1,0)$.\n",
    "\n",
    "iii) Fix the prototypes  and the assignments are as before.\n",
    "\n",
    "\n",
    "Thus, the points  $(-1,-1)$, $(-1,0)$, $(-1,1)$ are assigned to cluster 1, and the points $(1,-1)$, $(1,0)$, and $(1,1)$ to cluster 2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Problem 4) \n",
    "\n",
    "<img src=\"fig20.png\" width=\"200px\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4da75",
   "metadata": {},
   "source": [
    "## Dimension Reduction by PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a009814",
   "metadata": {},
   "source": [
    "```{ principal component analysis (PCA)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faffc63e",
   "metadata": {},
   "source": [
    "1)\n",
    "a)\n",
    "\n",
    "The data matrix is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{X}=\n",
    "\\begin{pmatrix}\n",
    "   -1 & 3\\\\\n",
    "0& 0 \\\\\n",
    "1&-3\n",
    "\\end{pmatrix}.\n",
    " \\end{align*}\n",
    " The covariance matrix is\n",
    " \\begin{align*}\n",
    "\\mathbf{V}=\\frac{1}{3}\n",
    "\\begin{pmatrix}\n",
    "   2 & -6 \\\\\n",
    "-6 & 18\\\\\n",
    "\\end{pmatrix}= \n",
    " \\begin{pmatrix}\n",
    "   2/3 & -2 \\\\\n",
    "-2 & 6\\\\\n",
    "\\end{pmatrix}\n",
    "  \\end{align*}\n",
    "Thus, $Var(\\mathbf{x})=2/3$, $Var(\\mathbf{y})=6$, and $Cov(\\mathbf{x},\\mathbf{y})=-2.$\n",
    "\n",
    "b) $\\,$The eigenvalues are 0 and 10. The principal component is determined by the largest eigenvalue $\\lambda=10$, which has eigenvector $(1,-3).$ This vector determines the line $y=-3x,$ which is the desired line.\n",
    "\n",
    "2)\n",
    "a)\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\scriptsize\n",
    "\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    " 2 & 2 & 0 \\\\\n",
    " 2 & 2 & 0 \\\\\n",
    " 0 & 0 &  1/2\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "b)\n",
    "\n",
    "The eigenvalues of $\\mathbf{A}$ are $\\lambda_1=4$, $\\lambda_2=1/2$, $\\lambda_3=0$, with corresponding unit eigenvectors for  $\\mathbf{v}_1=(1/\\sqrt{2},1/\\sqrt{2},0)^T$, $\\mathbf{v}_2=(0,0,1)^T$, and $\\mathbf{v}_3=(1/\\sqrt{2},-1/\\sqrt{2},0)^T$\n",
    "\n",
    "The first principal component of $\\mathbf{X}$ is given by $\\lambda_1,\\mathbf{v}_1$, the second principal component of  is given by $\\lambda_2,\\mathbf{v}_2$, and  the third principal component  is given by $\\lambda_3,\\mathbf{v}_3$. \n",
    "\n",
    "c)\n",
    "\n",
    "The variance of the data along the line through the origin determined by $\\mathbf{v}_1$ is the maximum among all lines through the origin.\n",
    "\n",
    "The total variance of the data (sum of squared distances to the origin) is the sum of the variances by projecting the data onto the three lines through the origin determined by the mutually orthogonal unit vectors $\\mathbf{v}_1$,  $\\mathbf{v}_2$, and  $\\mathbf{v}_3$.\n",
    "\n",
    "The variances along each of these lines decrease. Each captures more of the variance in an optimal way. The first principal component captures the most amount of variance projecting the data onto a line through the origin (1D subspace). This line is determined by $\\mathbf{v}_1$. The first two principal components capture the most variance if the data is projected onto a plane through the origin (2D subspace): the plane is determined by the vectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$.\n",
    "\n",
    "3)\n",
    "\n",
    "a) $\\,$ By inspection, the greatest variance occurs in the $y$-coordinate, so we expect the first principal component to be in the direction of the line determined by $(0,1,0)$. In the orthogonal direction (i.e., the $xz$-plane), we see that the data lie on the line $x=2z$. Thus, we  expect the second principal component to be in the direction of the line determined by $(2,0,1)/\\sqrt{5}$.\n",
    "\n",
    "b) $\\,$ \n",
    "\n",
    "$$\n",
    "\\mathbf{A}=\n",
    "\\begin{pmatrix}\n",
    " 2 & 0 & 1 \\\\\n",
    " 0 & 8 &  0 \\\\\n",
    " 1 & 0 &  1/2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The first principal component is given by $\\lambda_1=8$ with eigenvector $(0,1,0)$.\n",
    "\n",
    "The second principal component is given by eigenvector $\\lambda_2=5/2$ with eigenvector $(2,0,1)/\\sqrt{5}$.\n",
    "\n",
    "4)\n",
    "a) $\\,$ The line along which the projected points have the greatest variance.\n",
    "\n",
    "b) $\\,$ There is a difference in the ouliers--of significance in considering the most vulnerable.\n",
    "\n",
    "\n",
    "5)\n",
    "a) $\\,$ The original image is represented by a vector in $\\mathbf{R}^{32^2}=\\mathbf{R}^{1024}$ where each pixel value corresponds to a coefficient in the standard basis.  The PCA image can be represented using  just 2 coefficients corresponding to the first two PCA basis vectors.\n",
    "\n",
    "b)  If we used the standard basis, we would get the grayscale values for \n",
    "two of the $64^2$ pixels, and the rest of the pixels would have a value of 0 (i.e., appear as pure black.) There would be no way to distinguish whether the letter is an 'a' or 'b' using just two coordinates in the standard basis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37723834",
   "metadata": {},
   "source": [
    "## Binary Classification by SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d48ab",
   "metadata": {},
   "source": [
    "```{index} support vector machines (SVM)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798bf86",
   "metadata": {},
   "source": [
    "1)\n",
    "a) $\\,$ The hyperplane is the line $x_1+2x_2=1$\n",
    "\n",
    "b) $\\,$  The two half-spaces are the half-plane  $\\pi^+: x_1+2x_2>1$ and the half-plane $\\pi^-: x_1+2x_2<1.$\n",
    "\n",
    "2) \n",
    "a) $\\,$ For any $k\\neq 0$, we have$ \\mathbf{w}\\cdot\\mathbf{x}+b=0$ if and only if $ k(\\mathbf{w}\\cdot\\mathbf{x}+b)=0$ if and only if $ k\\mathbf{w}\\cdot\\mathbf{x}+kb=0$ \n",
    "\n",
    "b) $\\,$  The signed distance formula is $\\rho(\\mathbf{x},\\pi)=\\frac{\\mathbf{w}\\cdot\\mathbf{x}+b}{\\|\\mathbf{w}\\|}$. Since $\\mathbf{w}\\cdot\\mathbf{0}=0,$ the signed distance is $\\mathbf{\\rho}(\\mathbf{0},\\mathbf{\\pi})= \\frac{b}{\\|\\mathbf{w}\\|}$.\n",
    "\n",
    "c) $\\,$ As in the right panel of the 4th figure in \"Intuition Underlying SVM\", there are infinitely many parallel separating hyperplanes with the same margin filling in the space between support vectors. The SVM hyperplane is the one that is equidistant from the two classes $C_1, C_2.$ \n",
    "\n",
    "d) $\\,$ The SVM $L_2$ loss function simplifies to $J=\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2$ ($\\lambda>0$), which if minimized will then maximize the margin $\\frac{2}{\\|\\mathbf{w}\\|}$.\n",
    "\n",
    "3) See the JNB Support Vector Machines."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
