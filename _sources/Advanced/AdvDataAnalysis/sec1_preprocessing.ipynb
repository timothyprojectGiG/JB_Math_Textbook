{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # download punkt (if not already downloaded)\n",
    "nltk.download('stopwords', quiet=True) # download stopwords (if not already downloaded)\n",
    "\n",
    "# change this to your own data directory\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# read and preprocess data\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)\n",
    "text_df.drop(text_df.columns.values[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} preprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} natural language processing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) refers to the use of computers to process, analyze, and interpret human languages. NLP often involves the use of machine learning (ML), which is explained in more detail in [section 4](sec4_classification_algos.ipynb). Usually, natural language processors operate on both speech data and textual data. There are several components necessary to understand the structure and meaning of human language.\n",
    "\n",
    "From a linguistics perspective, it is important to look at the following:\n",
    " - **syntax**, the actual rules that define how words are combined to form understandable sentences;\n",
    " - **semantics**, referring to the meaning behind the phrases and sentences formed by syntax;\n",
    " - **morphology**, referring to the ways in which words can take different forms.\n",
    "\n",
    "Then, from a computational perspective, we can take the rules from these linguistics aspects and transform linguistic knowledge into rule-based and/or ML-based algorithms to solve problems related to natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.1\n",
    "Explore the Lexalytics NLP demo found at https://www.lexalytics.com/nlp-demo/. Choose any of the provided packs and look at the provided sample texts. \n",
    " * Look under Document and Themes. What words and phrases does the model highlight for each sample?\n",
    " * Look under Topics. What topics does the model detect in each sample? How accurate and comprehensive is its list?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} syntactic analysis\n",
    "```\n",
    "\n",
    "```{index} semantic analysis\n",
    "```\n",
    "\n",
    "```{index} keyword extraction\n",
    "```\n",
    "\n",
    "```{index} named entity resolution\n",
    "```\n",
    "\n",
    "```{index} text classification\n",
    "```\n",
    "\n",
    "```{index} sentiment analysis\n",
    "```\n",
    "\n",
    "```{index} email filtering\n",
    "```\n",
    "\n",
    "```{index} intent detection\n",
    "```\n",
    "\n",
    "```{index} language detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "Within the realm of preprocessing textual data, we first must perform certain detections and tasks; this ensures that we have textual data that make sense and are meaningful. The following all help at ensuring data cleanliness:\n",
    "\n",
    " - **Syntactic analysis:** Does the grammar make sense? Is the text gramatically correct?\n",
    " - **Semantic analysis:** Are we aware of the meaning of the words in context?  Do we understand the structure, word interaction, and related concepts of the text?\n",
    " - **Keyword extraction:** What are the most important words in the text?\n",
    " - **Named entity resolution:** How do we identify and extract entities (names, organizations, addresses and places) and relationships? \n",
    " - **Text classification:** Can we organize our text into predefined categories? Ways of doing this include:\n",
    "   - **Sentiment analysis:** Can we classify text (such as customer feedback) into positive and negative feedback?\n",
    "   - **Email filtering:** If we are taking our texts from emails, can we classify email text as spam mail and remove spam?\n",
    "   - **Intent detection:** What is the text generator trying to achieve? For example, searching “apple” could indicate an intent of buying, eating, or researching apples.\n",
    "   - **Language detection:** Can we classify the body of text into languages, with an associated probability?\n",
    "\n",
    ":::{admonition} Exercise 1.2\n",
    "Take another look at the Lexalytics NLP demo found at https://www.lexalytics.com/nlp-demo/. Choose any of the provided packs and look at the provided sample texts.\n",
    " * Look under Document and Themes, which present the model's sentiment analysis. What sentiment scores does the model give to the various words and phrases it has extracted? Do you agree or disagree with its verdicts?\n",
    ":::\n",
    "\n",
    "Looking at the UN SDG dataset, we find that it includes positive or negative labels. Since each text is classified with a single goal, positive labels indicate the agreement of that text with the goal, while negative labels indicate disagreement. We use `sum()` to sum the negative labels and the positive labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277524"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.labels_negative.sum() + text_df.labels_positive.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the number of text samples in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40062"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the sum of the positive and negative labels does not equal the number of texts in the dataset. This is because the texts have been labeled by more than one volunteer. As the table below shows, between 3 and 990 volunteers labeled each text, with an average (and median) of about 7 volunteers per text. The columns `labels_positive` and `labels_negative` give the total number of positive and negative labels assigned to each text by these volunteers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40062.000000\n",
       "mean         6.927363\n",
       "std         15.578931\n",
       "min          3.000000\n",
       "50%          7.000000\n",
       "max        990.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(text_df['labels_positive'] + text_df['labels_negative']).describe(percentiles=[0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, determining a text's agreement with the assigned SDG goal will be more complicated than simply reading the positive and negative labels. These observations thus provide some beginning rationale for the necessity of preprocessing.\n",
    "\n",
    "The data preprocessing we will need to perform includes some tasks specific to textual data, but it also includes some work that can be done on datasets more generally. For our UN SDG dataset, it is useful to remove the columns that we don't need, which for this dataset are any columns with N/A values in all entries. We can use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns removed: 0\n"
     ]
    }
   ],
   "source": [
    "# save the original number of columns in a variable\n",
    "ncols_old = text_df.shape[1]\n",
    "# remove columns comprised solely of N/A values\n",
    "text_df.dropna(axis=1, how='all', inplace=True)\n",
    "print(\"Number of columns removed:\", ncols_old - text_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there were no columns comprised solely of N/A values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a good idea to check our data for any other discrepancies, such as missing or incorrect entries. \n",
    "\n",
    ":::{admonition} Exercise 1.3\n",
    "* Modify the code above to remove any rows that contain only N/A values. (Hint: change the value of the `axis` parameter.) How many rows, if any, were removed?\n",
    "* Using the `isna()` and `any()` functions, check the data for any remaining N/A values. Is it a good idea to remove entries with N/A values?\n",
    ":::\n",
    "\n",
    "For our UN SDG dataset, we want texts that are clearly classified into a single goal. Positive labels indicate agreement with the labeled goal, while negative labels indicate disagreeement. With this in mind, our filtering will remove the rows with labeling agreement less than or equal to 0.5, as well as the rows where the number of positive labels minus the negative labels is less than or equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = text_df.query(\"agreement > 0.5 and (labels_positive - labels_negative) > 2\")\n",
    "text_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.4\n",
    "Use `text_df.info()` to examine information about the dataset after running the preprocessing in this section. What columns are included? What data types are there, and how many entries? Does this seem like a reasonable size of data to work with?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} levels of processing\n",
    "```\n",
    "\n",
    "```{index} characters\n",
    "```\n",
    "\n",
    "```{index} words\n",
    "```\n",
    "\n",
    "```{index} sentences\n",
    "```\n",
    "\n",
    "```{index} paragraphs\n",
    "```\n",
    "\n",
    "```{index} paragraphs\n",
    "```\n",
    "\n",
    "```{index} corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels of Processing\n",
    "\n",
    "In (almost) any language, there are distinct levels of a text we can focus on: **characters** are strung together to form **words**, which are put together into **sentences**, then **paragraphs**, **documents**, and a whole set of documents, or a **corpus**. In all these cases, we put multiples together to make the next level, and decisions in preprocessing at one level will inherently impact the next level.\n",
    "\n",
    "We will not be focusing on using algorithms to process whole documents or anything above that level, so we can look at preprocessing at the remaining levels: characters, words, sentences, and paragraphs. \n",
    "\n",
    "**For characters**, we typically process textual data by removing special characters, punctuations, and normalizations. However, this is not necessarily the case for every NLP approach. In some cases, taking away these special characters, punctuations, and normalizations will completely alter the meaning of higher levels of text.\n",
    "\n",
    "**For words**, we first have to define words within our text snippets. This is known as **segmenting**. In the English language, we can use white space to segment text into words. However, this is not always the case with other languages, so it is important to know the linguistic rules of the primary language you are working with. Additionally, we perform some removals and/or replacements for odd words such as abbreviations, acronyms, numbers, and misspelled words. Finally, we normalize our text data; this is typically done by **stemming** (covered later in this section) or **lemmatization**.\n",
    "\n",
    "**For sentences**, we first have to define sentence boundaries. In most languages, including English, sentences are started with a capital letter and ended with a period. However, periods and capital letters are also used within sentences, and capital letters or periods might not even exist in other languages, which again highlights the importance of knowing the language's rules.  Within sentences, we can also mark phrases. In English sentences, we can typically label phrases in sentences with a subject, predicate, and object; the subject is the one performing the action, or predicate, on or directed towards the object. Finally, we parse the sentence by tagging the words with their respective part of speech. Note that this can only be done on the sentence level, not the word level, as the same word can have a different part of speech depending on how it is used in a sentence.\n",
    "\n",
    "Finally, **for paragraphs**, our primary level of preprocessing is to understand the text by extracting the meaning of the text. This can include sentiments, emotions, intentions, etc. A good way of doing this is to perform abstractive summarization, which involves constructing new text that concisely captures the meaning of the old text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual process of text preprocessing, however, does not necessarily flow distinctly from one level to another. The order of preprocessing steps, as well as whether or not to include specific steps, depends largely on the application. An example process is given here: \n",
    "\n",
    " 1. Segmentation (breaking text into sentences)\n",
    " 2. Spelling correction\n",
    " 3. Noise removal (includes removal of text that would otherwise confuse the main text, including emojis, foreign language, and hyperlinks)\n",
    " 4. Language detection (identifying the language used in a body of text)\n",
    " 5. Stop-words removal (these words are typically high frequency, generic, and less context-specific)\n",
    " 6. Case-folding (removing variances in case such as lowercase, uppercase, titlecase, and so on)\n",
    " 7. Lemmatization\n",
    " 8. Tokenization (break texts into words, phrases, symbols, and other semantically useful units or meaningful elements)\n",
    " 9. Parsing (part of speech identification)\n",
    " 10. Standardization\n",
    " 11. Stemming (reduces tokens to base forms, or **stems**, and removes affixes)\n",
    "\n",
    "Many of these steps may be unfamiliar - that is perfectly fine, as they will be covered later in this section. \n",
    "\n",
    "It is commonly accepted that preprocessing text helps improve accuracy, so it is critical to perform good, appropriate preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} procedures\n",
    "```\n",
    "\n",
    "```{index} stemming\n",
    "```\n",
    "\n",
    "```{index} Porter stemmer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures\n",
    "\n",
    "### Stemming \n",
    "\n",
    "As mentioned previously, stemming is performed to help normalize our textual data by reducing tokens to their base forms. This is done so that we do not end up with many different words with very similar meaning. For example, stemming turns \"watch\", \"watched\", \"watching\", and \"watches\" into \"watch\". To do this, the algorithm accesses a database where known word forms, such as the various forms of \"watch\", are grouped together. It then analyzes the text for all instances of these forms and turns them all into the root word.\n",
    "\n",
    "Stemmers also typically perform preprocessing tasks like turning words not at the beginning of a sentence into lowercase and removing **stop words**, which are common words like \"the\" and \"that\" which show up in a variety of contexts and typically do not change meaning across these contexts. The `NLTK` library contains a set of English stop words, which the following code prints out. Notice how many of these stop words are common words like pronouns, prepositions, and conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming algorithms can be very useful for preprocessing text. However, there are always irregularities when using any stemming algorithm. Take the following examples of how a stemming algorithm might transform sentences: \n",
    "* \"I bought an Apple\" => *change to lowercase* => \"I bought an apple\"\n",
    "  * Here, the stemming algorithm has removed the capital on \"Apple\". We can clearly see that this drastically altered the meaning; now, the speaker has bought a fruit instead of a product from the company Apple.\n",
    "* \"I watched *The Who*\" => *remove stop words* => \"I watched\"\n",
    "  * Here, the stemming algorithm has removed the stop words \"the\" and \"who\", but this again changed the meaning of the sentence. Instead of watching the TV show *The Who*, the speaker has now watched something unspecified. \n",
    "* \"Take that!\" => *remove stop words* => \"Take\"\n",
    "  * Like the above example, the stemming algorithm removed a stop word, \"that\". However, the sentence now loses all meaning: it is no longer a complete sentence, and without the \"that\" which served to add a violent, fighting intent to the original sentence, this emotion and intent is completely gone.\n",
    "\n",
    "Developed by Martin Porter in 1980, the **Porter Stemmer** is the oldest and most commonly used stemming algorithm in many languages. It is slightly time consuming, so another stemming algorithm that can be used is the **Lancaster Stemmer**. This stemmer is very aggressive, however, and is especially harmful for short tokens, where the token itself may become unclear or altered to a point where it loses meaning.\n",
    "\n",
    "Let's try these stemmers, as well as a third type, the **Snowball Stemmer**, on some tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>narrative</th>\n",
       "      <th>comprehensives</th>\n",
       "      <th>publicised</th>\n",
       "      <th>moderately</th>\n",
       "      <th>prosperous</th>\n",
       "      <th>legitimizes</th>\n",
       "      <th>according</th>\n",
       "      <th>glorifies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Porter Stemmer</th>\n",
       "      <td>narr</td>\n",
       "      <td>comprehens</td>\n",
       "      <td>publicis</td>\n",
       "      <td>moder</td>\n",
       "      <td>prosper</td>\n",
       "      <td>legitim</td>\n",
       "      <td>accord</td>\n",
       "      <td>glorifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <td>narrat</td>\n",
       "      <td>comprehens</td>\n",
       "      <td>publicis</td>\n",
       "      <td>moder</td>\n",
       "      <td>prosper</td>\n",
       "      <td>legitim</td>\n",
       "      <td>accord</td>\n",
       "      <td>glorifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lancaster Stemmer</th>\n",
       "      <td>nar</td>\n",
       "      <td>comprehend</td>\n",
       "      <td>publ</td>\n",
       "      <td>mod</td>\n",
       "      <td>prosp</td>\n",
       "      <td>legitim</td>\n",
       "      <td>accord</td>\n",
       "      <td>glor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  narrative comprehensives publicised moderately prosperous  \\\n",
       "Porter Stemmer         narr     comprehens   publicis      moder    prosper   \n",
       "Snowball Stemmer     narrat     comprehens   publicis      moder    prosper   \n",
       "Lancaster Stemmer       nar     comprehend       publ        mod      prosp   \n",
       "\n",
       "                  legitimizes according glorifies  \n",
       "Porter Stemmer        legitim    accord   glorifi  \n",
       "Snowball Stemmer      legitim    accord   glorifi  \n",
       "Lancaster Stemmer     legitim    accord      glor  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens =  ['narrative', 'comprehensives', 'publicised', 'moderately', 'prosperous', 'legitimizes', 'according', 'glorifies'] \n",
    "\n",
    "porter = nltk.stem.porter.PorterStemmer() # Porter stemmer\n",
    "snowball = nltk.stem.snowball.EnglishStemmer() # Snowball stemmer\n",
    "lancaster = nltk.stem.lancaster.LancasterStemmer() # Lancaster stemmer\n",
    "\n",
    "# apply each stemmer to the tokens\n",
    "porter_results = [porter.stem(i) for i in tokens]\n",
    "snowball_results = [snowball.stem(i) for i in tokens]\n",
    "lancaster_results = [lancaster.stem(i) for i in tokens]\n",
    "\n",
    "# display results\n",
    "pd.DataFrame(\n",
    "    index = ['Porter Stemmer', 'Snowball Stemmer', 'Lancaster Stemmer'],\n",
    "    columns = tokens,\n",
    "    data = [porter_results, snowball_results, lancaster_results]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.5\n",
    "How do the different stemmers compare for the list of words in the above example? Which stemmer might have the most trouble on a larger set of tokens?\n",
    ":::\n",
    "\n",
    ":::{admonition} Exercise 1.6\n",
    "Modify the above code to stem your own tokens. How are words commonly shortened, and what types of words seem to cause the most trouble for the stemmers?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} tokenizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "\n",
    "The primary goal of a tokenizer is to break text into tokens. This is highly language dependent; in English, we use white space and punctuation as word and token separators, but this may not work in some languages. Since tokens are semantically useful units of text, a token is not necessarily the same as a word; in fact, some tokens may be pieces of words, or even symbols such as punctuation.\n",
    "\n",
    "For example, the following code uses `NLTK` to tokenize one of the texts from our UN SDG dataset. Note how not only words like \"policy\" but also numbers like \"70\", suffixes like \"-'s\", and symbols like \"%\" and \"(\" have been separated into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \"About 70% of them have reported at least one trade policy targeting women's economic empowerment. Overall, in four years, almost half of the WTO membership has implemented trade policies in support of women (at least one). It simply provides examples of trade policies as reported by WTO members themselves.\"\n",
      "\n",
      "Tokens: ['About', '70', '%', 'of', 'them', 'have', 'reported', 'at', 'least', 'one', 'trade', 'policy', 'targeting', 'women', \"'s\", 'economic', 'empowerment', '.', 'Overall', ',', 'in', 'four', 'years', ',', 'almost', 'half', 'of', 'the', 'WTO', 'membership', 'has', 'implemented', 'trade', 'policies', 'in', 'support', 'of', 'women', '(', 'at', 'least', 'one', ')', '.', 'It', 'simply', 'provides', 'examples', 'of', 'trade', 'policies', 'as', 'reported', 'by', 'WTO', 'members', 'themselves', '.']\n"
     ]
    }
   ],
   "source": [
    "example = text_df.loc[2425, 'text']\n",
    "print(f'Original text: \"{example}\"\\n')\n",
    "print('Tokens:', nltk.word_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.7\n",
    "Modify the above code to tokenize your own text. Does anything surprise you about how the tokenizer breaks up the text? What makes the resultant tokens semantically useful or meaningful?\n",
    ":::\n",
    "\n",
    "For the purposes of building ML models, there are several things that tokenizers track in addition to creating the tokens themselves. Tokenizers track *features*, or the frequency of occurrence of each individual token (normalized or not). They also produce a *sample*, or a vector of all the token frequencies for a given document. \n",
    "\n",
    "When a tokenizer vectorizes a corpus, it counts the occurrences of tokens in each document, normalizes and weights these occurrences, and constructs an n-gram or other representation of the text, which is discussed in the [next section](sec2_transform_features.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} named entity recognition (NER)\n",
    "```\n",
    "\n",
    "\n",
    "```{index} Allen NLP NER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "The purpose of named entity recognition (NER) is to identify named entities in a text, extract them, and classify them according to their type (people, locations, organizations, and so on). For example, the following code uses the `spaCy` library to recognize named entities in one of the texts from the UN SDG dataset and then displays them with `spaCy`'s built-in `displacy` visualizer.\n",
    "\n",
    ":::{note}\n",
    "Before running the following code, you may have to download `en_core_web_sm`. This is a trained pipeline which `spaCy` uses to perform the NLP tasks we've been discussing on English-language texts. You can download it by uncommenting the second line in the code below.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Source: \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    OECD Development Centre\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", based on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IEA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (2015a), \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Energy Outlook 2015\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IEA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015b\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "), \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Energy Outlook\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ": Special Report on \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Southeast Asia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". By definition, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TPES\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is equal to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Total Primary Energy Demand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (TPED), and includes power generation, other energy sector and total final energy consumption (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IEA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", 2015a). \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    China\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " will continue to account for the largest share of the energy demand in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emerging Asia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", even though its share of the region’s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TPES\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " decreases from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    69%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2013\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    57%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2040\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " owing to the strong growth in energy demand from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ASEAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## uncomment the next line to download the 'en_core_web_sm' pipeline\n",
    "# ! python3 -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sample = text_df.loc[5707, 'text']\n",
    "doc = nlp(sample)\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the meaning of these labels may not be clear. We can use the `spacy.explain()` function to learn more about the meaning of each label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENT: Percentage, including \"%\"\n",
      "GPE: Countries, cities, states\n",
      "LOC: Non-GPE locations, mountain ranges, bodies of water\n",
      "DATE: Absolute or relative dates or periods\n",
      "ORG: Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "labels = set([ent.label_ for ent in doc.ents])\n",
    "for label in labels:\n",
    "    print(f'{label}: {spacy.explain(label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the trained model has automatically identified and classified various entities in the text. Notice how the types of entities range from countries and organizations to dates and percentages.\n",
    "\n",
    "::::{admonition} Exercise 1.8\n",
    "Use the following code to perform NER on a random sample from the UN SDG dataset. Rerun the code several times to get different samples. What types of entities does the model identify?\n",
    "\n",
    "```\n",
    "sample = nlp(text_df['text'].sample(1).values[0])\n",
    "doc = nlp(sample)\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
    "```\n",
    "::::\n",
    "\n",
    "Although the trained model has correctly identified many of the named entities in this text, it has still made some mistakes. For example, it incorrectly labels \"Total Primary Energy Demand\" as an organization. Additionally, the model often relies on capitalization to identify entities. If we change the sample text to lowercase, notice how the model fails to recognize many of the entities that it previously identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">source: \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    oecd development centre\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", based on iea (2015a), world energy outlook \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", iea (\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015b\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "), world energy outlook \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ": special report on \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    southeast asia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". by definition, tpes is equal to total primary energy demand (tped), and includes power generation, other energy sector and total final energy consumption (iea, 2015a). \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    china\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " will continue to account for the largest share of the energy demand in emerging \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    asia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", even though its share of the region’s tpes decreases from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    69%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2013\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    57%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2040\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " owing to the strong growth in energy demand from \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    asean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    india\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(sample.lower())\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.9\n",
    "Modify the code from Exercise 1.8 to perform NER on a random sample from the UN SDG dataset, first in the original case and then in lowercase. (Hint: use the `lower()` function to make the sample lowercase.) Rerun your code several times to get different samples. What mistakes does the model tend to make? What types of entities does it tend to misidentify when the text is converted to lowercase?:::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} string operations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Operations\n",
    "\n",
    "Now that we know about various preprocessing techniques, we can review some basic string operations. These will also be important in identifying various sentences and tokens within our dataset in case we would like to focus on specific words. They can also help in identifying cases, numeric characters, and other tests that we might need to conduct. A few are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "s.startswith(t) # test if s starts with t\n",
    "s.endswith(t) # test if s ends with t\n",
    "t in s # test if t is a substring of s\n",
    "s.islower() # test if s contains cased characters and all are lowercase\n",
    "s.isupper() # test if s contains cased characters and all are uppercase\n",
    "s.isalpha() # test if s is non-empty and all characters in s are alphabetic\n",
    "s.isalnum() # test if s is non-empty and all characters in s are alphanumeric\n",
    "s.isdigit() # test if s is non-empty and all characters in s are digits\n",
    "s.istitle() # test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following code uses the `startswith()` function to test if the first text from the UN SDG dataset begins with the string 'The'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.loc[0, 'text'].startswith('The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above are all called on one string at a time. In `pandas`, many of these string operations can also be performed on an entire series or dataframe, not just a single value. (These are known as *vectorized* functions.) For example, the following code uses `str.startswith`, the vectorized version of `startswith`, to test which texts from the UN SDG dataset begin with the string 'The'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "4        False\n",
       "         ...  \n",
       "24664    False\n",
       "24665    False\n",
       "24666    False\n",
       "24667    False\n",
       "24668     True\n",
       "Name: text, Length: 24669, dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df['text'].str.startswith('The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 1.10\n",
    "Practice using some of the string operations on your own sample sentences and observe the results.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example to work through, let's take the Chinese phrase '四个全面' and see if we can locate it in the texts. This phrase, anglicized as \"sigequanmian\" and roughly translating to \"four comprehensives\", was publicized by Chinese President Xi Jinping as a set of goals for China. We will use the vectorized `str.contains` function to search the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sdg</th>\n",
       "      <th>labels_negative</th>\n",
       "      <th>labels_positive</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18009</th>\n",
       "      <td>10.32890/UUMJLS.6.2015.4584</td>\n",
       "      <td>baa468803d1c7dba3c1096b1665ed7f7</td>\n",
       "      <td>In China today,President Xi Jinping’s new gran...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doi                           text_id  \\\n",
       "18009  10.32890/UUMJLS.6.2015.4584  baa468803d1c7dba3c1096b1665ed7f7   \n",
       "\n",
       "                                                    text  sdg  \\\n",
       "18009  In China today,President Xi Jinping’s new gran...   16   \n",
       "\n",
       "       labels_negative  labels_positive  agreement  \n",
       "18009                1                4        0.6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[text_df['text'].str.contains('四个全面')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible string operation is comparison, as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t1 = text_df[text_df['text'].str.contains('四个全面')][\"text\"].values[0]\n",
    "# compare using '=='\n",
    "print(t1 == text_df.iloc[18009].text)\n",
    "# compare casefolded version\n",
    "print(t1 == text_df.iloc[18009].text.casefold())\n",
    "# compare lowercase and casefold\n",
    "print(t1.lower() == text_df.iloc[18009].text.casefold())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Python uses strict comparisons, meaning that the two sentences will not be equal unless the cases (capital and lowercase) are matched. Note that in the above example, `lower()` and `casefold()` yielded equal results. However, this is not the case for all languages. Take the German letter ß:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lower(): groß\n",
      "Using casefold(): gross\n",
      "groß == gross: False\n"
     ]
    }
   ],
   "source": [
    "text = 'groß'\n",
    "\n",
    "# convert text to lowercase using lower()\n",
    "print('Using lower():', text.lower())\n",
    "\n",
    "# convert text to lowercase using casefold()\n",
    "print('Using casefold():', text.casefold())\n",
    "\n",
    "# check equality\n",
    "print(f'{text.lower()} == {text.casefold()}:', text.lower() == text.casefold())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German word *groß* translates as \"large\" or \"big\", and this meaning is kept using `lower()`. However, when using `casefold()`, the word becomes *gross*, which is an acceptable alternate spelling in German but is also used in English to indicate disgust. Even though *groß* and *gross* represent the same word in German, Python's strict comparison will treat them as different. This is another reason to perform consistent and appropriate preprocessing of text.\n",
    "\n",
    "It is up to you to decide which of these functions to use when converting to lowercase; however, it is best to keep this consistent across all your code so as to avoid potential problems such as this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
