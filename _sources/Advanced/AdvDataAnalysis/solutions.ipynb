{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c23635",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # download punkt (if not already downloaded)\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# change this to your own data directory\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# read and preprocess data\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)\n",
    "text_df.drop(text_df.columns.values[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b0513",
   "metadata": {},
   "source": [
    "# Solutions to Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a4bed",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8fd66",
   "metadata": {},
   "source": [
    "**Exercise 1.1**\n",
    "\n",
    "Answers may vary.\n",
    "\n",
    "**Exercise 1.2**\n",
    "\n",
    "Answers may vary.\n",
    "\n",
    "**Exercise 1.3**\n",
    "The following code removes any rows that contain only N/A values. In this case, there are no such rows to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481928bf-939f-4b7c-84d7-02858271201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "nrows_old = text_df.shape[0]\n",
    "text_df.dropna(axis=0, how='all', inplace=True)\n",
    "print(\"Number of rows removed:\", nrows_old - text_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d0090-88ce-4d17-8be0-317493c0a5cc",
   "metadata": {},
   "source": [
    "The next line of code checks for the existence of any remaining N/A values. It turns out that there are none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ced324a-f4ee-4f87-8ae7-1519afc5b866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doi                False\n",
       "text_id            False\n",
       "text               False\n",
       "sdg                False\n",
       "labels_negative    False\n",
       "labels_positive    False\n",
       "agreement          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa2651-fa3a-4da4-8510-dc24be08f7fe",
   "metadata": {},
   "source": [
    "Whether or not entries with N/A values should be removed depends on the dataset and the nature of the problem. Sometimes, entries with N/A values should be dropped, while at other times, they should be kept unchanged, or replaced with interpolated or placeholder values. Consult [the `pandas` documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html) for more information about how to deal with missing values in dataframes.\n",
    "\n",
    "**Exercise 1.4**\n",
    "\n",
    "After filtering the dataset, we inspect it using the `info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b5fd3d-9258-4061-8068-7ca371ae56a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24669 entries, 0 to 24668\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   doi              24669 non-null  object \n",
      " 1   text_id          24669 non-null  object \n",
      " 2   text             24669 non-null  object \n",
      " 3   sdg              24669 non-null  int64  \n",
      " 4   labels_negative  24669 non-null  int64  \n",
      " 5   labels_positive  24669 non-null  int64  \n",
      " 6   agreement        24669 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# filter the dataset\n",
    "text_df = text_df.query(\"agreement > 0.5 and (labels_positive - labels_negative) > 2\")\n",
    "text_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# inspect it\n",
    "text_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631456e-d47a-4d00-b412-d3345bc047da",
   "metadata": {},
   "source": [
    "We have 40062 entries with 7 features (see [section 0](sec0_data.ipynb) for details). The data types range from `object` (likely denoting strings) to `int64` (integers) to `float64` (floating-point numbers). This is a reasonable amount of data to work with.\n",
    "\n",
    "**Exercise 1.5**\n",
    "\n",
    "The Porter and Snowball stemmers are largely comparable, while the Lancaster stemmer is the most aggressive. As a result, the Lancaster stemmer is likely to have the most trouble on a larger set of tokens.\n",
    "\n",
    "**Exercise 1.6**\n",
    "\n",
    "Answers may vary. Some possible observations include the fact that stemmers tend to remove affixes (such as `-ing`, `-ed`, and `-s` in English) and the fact that irregular words are particularly likely to give the stemmers trouble.\n",
    "\n",
    "**Exercise 1.7**\n",
    "\n",
    "Answers may vary.\n",
    "\n",
    "**Exercise 1.8**\n",
    "\n",
    "Answers may vary. Some possible entity labels include `GPE` (\"nationalities or religious or political groups\"), `TIME` (\"times smaller than a day\"), `QUANTITY` (\"measurements, as of weight or distance\"), and `WORK_OF_ART` (\"titles of books, songs, etc.\").\n",
    "\n",
    "**Exercise 1.9**\n",
    "\n",
    "Sample code solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f24f495-a6ca-4925-8db9-e1ee6d22b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL CASE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The legal protection of religious freedom in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " has been subject to significant debate over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    recent years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last four years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " this question has formed the basis of inquiries by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Australian Law Reform Commission\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", a \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Parliamentary Committee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", as well as a specially formed \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Expert Panel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", chaired by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Philip Ruddock\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". In this article we outline the international and comparative approach taken to protect freedom of religion, and contrast this to the position in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". We find that \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " law does not adequately protect this foundational human right. We then assess the recommendations proposed by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Ruddock Review\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". We argue that although \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Expert Panel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " recognised the extent of the problem, it did not propose a comprehensive or holistic solution that will resolve existing inadequacies. To protect religious freedom, and indeed human rights more generally, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Commonwealth Parliament\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " should enact a national human rights act</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOWERCASE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the legal protection of religious freedom in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " has been subject to significant debate over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    recent years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last four years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " this question has formed the basis of inquiries by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    australian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " law reform commission, a parliamentary committee, as well as a specially formed expert panel, chaired by philip ruddock. in this article we outline the international and comparative approach taken to protect freedom of religion, and contrast this to the position in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". we find that \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    australian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " law does not adequately protect this foundational human right. we then assess the recommendations proposed by the ruddock review. we argue that although the expert panel recognised the extent of the problem, it did not propose a comprehensive or holistic solution that will resolve existing inadequacies. to protect religious freedom, and indeed human rights more generally, the commonwealth parliament should enact a national human rights act</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load trained pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# perform NER on random sample in both original and lower case\n",
    "sample = text_df['text'].sample(1).values[0]\n",
    "doc = nlp(sample)\n",
    "print('ORIGINAL CASE')\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)\n",
    "print('\\nLOWERCASE')\n",
    "doc = nlp(sample.lower())\n",
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f2a9b-8597-410d-bcb1-5ac8f242263d",
   "metadata": {},
   "source": [
    "Answers may vary depending on the samples chosen. This sample demonstrates that the model sometimes confuses organizations with people. Additionally, it shows that the model often fails to recognize organization names (especially abbreviated ones) when they are converted to lowercase.\n",
    "\n",
    "**Exercise 1.10**\n",
    "\n",
    "Answers may vary.\n",
    "\n",
    "## About Text Data\n",
    "\n",
    "**Exercise 2.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2d4695-0833-4705-a6b9-69fdae1cb080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: 'the'\n"
     ]
    }
   ],
   "source": [
    "# get document-term matrix\n",
    "docs = text_df.text\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(docs)\n",
    "\n",
    "# get feature names and total counts\n",
    "feature_names = cv.get_feature_names_out()\n",
    "total_counts = cv_fit.sum(axis=0)\n",
    "\n",
    "# get the index of the most frequent word\n",
    "most_freq_feature = total_counts.argmax()\n",
    "\n",
    "# get the most frequent word itself\n",
    "most_freq_token = feature_names[most_freq_feature]\n",
    "print(f\"Most frequent word: '{most_freq_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81dfaa-a53a-420d-b105-28abc73e2565",
   "metadata": {},
   "source": [
    "**Exercise 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f103aff8-d450-440b-ba3a-de5fe3a8f996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the original vocabulary (with stop words): 45738\n",
      "Length of the new vocabulary (without stop words): 45440\n",
      "Number of stop words: 318\n",
      "Difference between original and new vocabularies: 298\n"
     ]
    }
   ],
   "source": [
    "# get document-term matrix with stop words removed\n",
    "cv2 = CountVectorizer(stop_words='english') # exclude English stop words\n",
    "cv2_fit = cv2.fit_transform(text_df.text)\n",
    "\n",
    "original_len = len(cv.vocabulary_) # length of the original vocabulary (with stop words)\n",
    "new_len = len(cv2.vocabulary_) # length of the new vocabulary (without stop words)\n",
    "stopwords = cv2.get_stop_words()\n",
    "\n",
    "print('Length of the original vocabulary (with stop words):', original_len)\n",
    "print('Length of the new vocabulary (without stop words):', new_len)\n",
    "print('Number of stop words:', len(stopwords))\n",
    "print('Difference between original and new vocabularies:', original_len - new_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd116a-fb43-471e-baf8-75ce5de0081b",
   "metadata": {},
   "source": [
    "The difference between the original and new vocabularies is less than the number of stop words. This is because not all of the stop words actually occur in the original vocabulary. The following code lists the stop words that are missing from the original vocabulary. Note how the difference between the original and new vocabulary lengths (298) added to the number of missing stopwords (20) is equal to the total number of stop words (318)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6305ef4d-bca6-4cf9-9eab-15ddc45bdd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 missing stopwords: {'whereafter', 'whence', 'noone', 'thereupon', 'i', 'thence', 'a', 'latterly', 'yours', 'whereupon', 'couldnt', 'whoever', 'anyhow', 'hasnt', 'whither', 'hers', 'amoungst', 'hereupon', 'yourselves', 'beforehand'}\n"
     ]
    }
   ],
   "source": [
    "missing_stopwords = stopwords - cv.vocabulary_.keys()\n",
    "print(f'{len(missing_stopwords)} missing stopwords:', missing_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeda29b-a5cb-48b3-8cac-9d8247377d62",
   "metadata": {},
   "source": [
    "**Exercise 2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd75863-ca03-43b1-888e-b518488240e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: 'countries'\n"
     ]
    }
   ],
   "source": [
    "# get feature names and total counts\n",
    "feature_names = cv2.get_feature_names_out()\n",
    "total_counts = cv2_fit.sum(axis=0)\n",
    "\n",
    "# get the index of the most frequent word\n",
    "most_freq_feature = total_counts.argmax()\n",
    "\n",
    "# get the most frequent word itself\n",
    "most_freq_token = feature_names[most_freq_feature]\n",
    "print(f\"Most frequent word: '{most_freq_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e6cc6-0433-4ef7-9dac-9864b5e1954c",
   "metadata": {},
   "source": [
    "**Exercise 2.4**\n",
    "\n",
    "First, we fit the one-hot encoder to the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a75d16-bfd5-48d8-8f01-eecb46d4776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = text_df.text.iloc[12737].lower()\n",
    "tokens = nltk.word_tokenize(sample_text)\n",
    "\n",
    "def ohe_reshape(tokens):\n",
    "    return np.asarray(tokens).reshape(-1,1)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore') # encode unknown tokens as vectors of all zeros\n",
    "ohe.fit(ohe_reshape(tokens));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313f389-048d-4f28-82e7-a40faad04262",
   "metadata": {},
   "source": [
    "Next, we transform each token only once by using a set to remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df97f920-d0fc-4d28-9946-0ad218789696",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = list(set(tokens))\n",
    "encodings = ohe.transform(ohe_reshape(token_set)).toarray() # encode the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c4ba5-f5b6-42b5-bb98-467fd7747353",
   "metadata": {},
   "source": [
    "There are multiple ways to check that the resulting encodings are unique, but one simple way is to use the `pandas` library. The following code transforms the encodings into a `pandas` dataframe and then verifies that there are no duplicates. This confirms that each learned token has a unique encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7ad9a8-78e1-4603-b7d6-3eb22a21e9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encodings).duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395bcd7-a9b2-4abe-9071-c60f05dc5146",
   "metadata": {},
   "source": [
    "**Exercise 2.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e85b56fe-ce76-421e-81fe-0174b57b1ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDG: 5\n",
      "Text: \"Female economic activities were critically examined and new light was shed on existing conceptions of traditional housework. Oxford University Press, 2007). An edited version of Ihe chapter is available al www.rci.rutgers.edu/~cwgl/globalcenler/charlotte/UN-Handbook.pdf. Targets were also set for the improvement of women's access to economic, social and cultural rights, including improvements in health, reproductive services and sanitation. The women in development approach is embodied in article 14 of the Convention, which focuses on rural women and calls on States to ensure that women \"\"participate in and benefit from rural development\"\" and also that they \"\"participate in the elaboration and implementation of development planning at all levels\"\".15 Participation is an important component of the right to development, as discussed below.\"\n"
     ]
    }
   ],
   "source": [
    "print('SDG:', text_df.sdg.iloc[118])\n",
    "print('Text:', text_df.text.iloc[118])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a37dd8-c2e4-4c0a-a921-d4e69d11899a",
   "metadata": {},
   "source": [
    "The most frequent words are \"women\" and \"development\", which occur 4 times each. This, together with the label of SDG 5 (gender equality), suggests that this document is about equality for women.\n",
    "\n",
    "**Exercise 2.6**\n",
    "\n",
    "Each token in a given document, except for the first and last, is grouped into two different bigrams (one with the previous token, and another with the next token). In this case, the large number of distinct bigrams in the entire corpus likely leads to a bigram vocabulary that is larger than the corresponding unigram vocabulary. However, many of the unigrams may occur more often than many of the bigrams do, making the total count of bigrams smaller than the total count of unigrams.\n",
    "\n",
    "**Exercise 2.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1617a64-42c3-4f53-b456-4a1cda57340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of trigrams (without stop words): 1301713\n",
      "Number of unique trigrams (without stop words): 1214215\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english') \n",
    "count_vector = count_vectorizer.fit_transform(docs)\n",
    "print('Total count of trigrams (without stop words):', count_vector.sum())\n",
    "print('Number of unique trigrams (without stop words):', len(count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103d0d0-d1fa-403a-bdcc-1b84c5d79f63",
   "metadata": {},
   "source": [
    "The total count of trigrams is smaller than the total count of bigrams, but the number of unique trigrams is larger than the total number of unique bigrams. The explanation for this is similar to the reasoning offered in the solution to the previous exercise, but substituting bigrams for unigrams and trigrams for bigrams.\n",
    "\n",
    "**Exercise 2.8**\n",
    "\n",
    "Answers may vary depending on the sentences chosen.\n",
    "\n",
    "**Exercise 2.9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d203be99-3ded-446d-a0b4-e741ca8680d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7223230490018149, recall = 0.8274428274428275, f1 = 0.7713178294573643\n"
     ]
    }
   ],
   "source": [
    "tp = 398\n",
    "fp = 153\n",
    "fn = 83\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall)/(precision + recall)\n",
    "\n",
    "print(f'Precision = {precision}, recall = {recall}, f1 = {f1}')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
