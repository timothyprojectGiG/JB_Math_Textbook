{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-ouput"
    ]
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# change this to your own data directory\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# read and preprocess data\n",
    "text_file_name = \"osdg-community-data-v2023-01-01.csv\"\n",
    "text_df = pd.read_csv(data_dir + text_file_name,sep = \"\\t\",  quotechar='\"')\n",
    "col_names = text_df.columns.values[0].split('\\t')\n",
    "text_df[col_names] = text_df[text_df.columns.values[0]].apply(lambda x: pd.Series(str(x).split(\"\\t\")))\n",
    "text_df = text_df.astype({'sdg':int, 'labels_negative': int, 'labels_positive':int, 'agreement': float}, copy=True)\n",
    "text_df.drop(text_df.columns.values[0], axis=1, inplace=True)\n",
    "text_df = text_df.query(\"agreement > 0.5 and (labels_positive - labels_negative) > 2\")\n",
    "text_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} document embedding\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding\n",
    "\n",
    "In the previous section, we represented variable length texts as fixed length numeric vectors; the approach we have used so far is the traditional approach of Bag of Words (BoW), which tokenizes a text into words (tokens). BoW ignores the order of the tokens, though it may pay attention to the frequency. This approach is high dimension, and very sparse; this may result in overfitting and high time complexity.\n",
    "\n",
    "A more modern text vectorization approach is **word embedding** (also called simply embedding), relying on neural representations. This approach takes distributional semantics into account; that is, a word’s meaning is inferred from the words that frequently appear close-by. Hence, we can construct a word’s context by using the set of words that appear nearby within a fixed-sized window. \n",
    "\n",
    "Semantically similar texts, then, would appear closer to each other in the vector space. We could also possibly capture semantic relationships by operations in the vector space; for example, similarity between texts could be measured by vector dot product. We could also perform algebraic operations; for example, \n",
    "\n",
    "$\\text{vector(``King'')} - \\text{vector(``Man'')} + \\text{vector(``Woman'')} \\sim \\text{vector(``Queen'')}$. \n",
    "\n",
    "Modern-day representations are typically learned from vast body of texts, often with deep neural networks, and they typically result in pre-trained models.\n",
    "\n",
    "To get embeddings, we use the `tensorflow` library, installed and imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "remove-output": true
   },
   "outputs": [],
   "source": [
    "## uncomment the next two lines of code to install tensorflow\n",
    "# ! pip install tensorflow\n",
    "# ! pip install tensorflow_hub\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "The `tensorflow` package can be finicky, especially on certain distributions of Python and on computers that are not very powerful. However, it is a versatile and useful package for document embedding, which is an important part of natural language processing. If you encounter trouble with `tensorflow`, try using a kernel on a different version of Python (such as Python 3.9) and/or a reduced version of the dataset. If necessary, the code from this section can be skipped, and later sections will not depend on code from this one.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the tokenizer found in the `nltk.data` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we first need to break our document corpus into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df_sentence = []\n",
    "text_df_sdg = []\n",
    "text_df_sample = []\n",
    "for (text, sdg, i) in iter(zip(text_df.text, text_df.sdg, text_df.index)):\n",
    "    sentences = sent_tokenize(text) \n",
    "    text_df_sentence += sentences\n",
    "    text_df_sdg += [sdg]*len(sentences)\n",
    "    text_df_sample += [i]*len(sentences)\n",
    "sentence_df = pd.DataFrame({\"text\": text_df_sentence, \"sdg\": text_df_sdg, \"sample\": text_df_sample})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `sentence_df` now contains all of the sentences in the corpus. The column `text` gives the sentences, the column `sdg` gives the SDG, and the column `sample` gives the index of the original text sample from which the sentence was extracted. We can use the `head` function to see the first few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sdg</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"From a gender perspective, Paulgaard points o...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But the fact that young people are still worki...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Paulgaard refers to continuity with tradi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As described earlier, Paulgaard (2015) conclud...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The average figure also masks large difference...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The number of annual contacts ranges from 2.0 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In addition, poor coverage of outpatient presc...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>These findings are consistent with previous wo...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Returns to education are also found to be an i...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In these countries, however, the magnitude of ...</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sdg  sample\n",
       "0  \"From a gender perspective, Paulgaard points o...    5       0\n",
       "1  But the fact that young people are still worki...    5       0\n",
       "2  When Paulgaard refers to continuity with tradi...    5       0\n",
       "3  As described earlier, Paulgaard (2015) conclud...    5       0\n",
       "4  The average figure also masks large difference...    3       1\n",
       "5  The number of annual contacts ranges from 2.0 ...    3       1\n",
       "6  In addition, poor coverage of outpatient presc...    3       1\n",
       "7  These findings are consistent with previous wo...   10       2\n",
       "8  Returns to education are also found to be an i...   10       2\n",
       "9  In these countries, however, the magnitude of ...   10       2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 3.1\n",
    "What are the dimensions of the `text_df` and `sentence_df` dataframes? What do each of the dimension numbers represent?\n",
    ":::\n",
    "\n",
    ":::{admonition} Exercise 3.2 \n",
    "Evaluate the performance of the sentence tokenizer by picking one of the sample texts, manually breaking it into sentences, and determining whether your sentence divisions match the corresponding ones in `sentence_df`.\n",
    ":::\n",
    "\n",
    "An important question to ask is how many sentences each document has; in other words, what is the distribution of the number of sentences in each text? We can determine that by *grouping* the dataframe by sample, counting the number of sentences in each group, and then counting those counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "3     12015\n",
       "4      7579\n",
       "5      2263\n",
       "6      1586\n",
       "2       586\n",
       "7       354\n",
       "8       119\n",
       "1        89\n",
       "9        21\n",
       "10       14\n",
       "12       12\n",
       "13        6\n",
       "15        6\n",
       "11        4\n",
       "14        4\n",
       "16        3\n",
       "17        2\n",
       "18        1\n",
       "24        1\n",
       "31        1\n",
       "21        1\n",
       "19        1\n",
       "20        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.groupby(by='sample').count()['text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the vast majority of the documents have less than 10 sentences, with most being only 3 or 4 sentences long.\n",
    "\n",
    "This type of sentence tokenization can help us with further NLP tasks down the line. We can now turn to the embedding method itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{index} universal sentence encoder (USE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder\n",
    "\n",
    "The Universal Sentence Encoder (USE) was first published by Google around 2018. It maps a sentence, word, or short paragraph to a fixed length (typically 512) numeric vector. This approach would mean semantically similar sentences would be placed closer to each other in the embedding space. \n",
    "\n",
    "Embeddings are typically the result of using raw text, so no pre-processing would be involved. This sentence embedding can then be used for downstream applications such as classification, clustering, and language prediction. \n",
    "\n",
    "USE is a pre-trained model trained on variety of data such as books and Wikipedia. It was trained with a deep averaging network (DAN) encoder; more information about the process behind USE can be found at https://arxiv.org/pdf/1803.11175.pdf.\n",
    "\n",
    "To utilize USE, we can take one of three approaches:\n",
    "\n",
    "1. We could take our desired document, turn it into a collection of sentences, and then map each sentence to its respective vector;\n",
    "2. We could treat each document as a short paragraph and match each document to its respective vector, or;\n",
    "3. We could take a similar approach to #1, except then aggregate the vectors for each document to form a single vector per document.\n",
    "\n",
    "USE can be found at https://tfhub.dev/google/universal-sentence-encoder/4. Go to this link, download the embedding (which is around 1GB in size), and put it in a directory you can access. Change the next line of code to refer to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your own embedding directory\n",
    "embedding_dir = \"embeddings/\"\n",
    "\n",
    "# load the embedding\n",
    "embed = hub.load(embedding_dir + \"universal-sentence-encoder_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first time you run this, it may take some time (5+ minutes) to complete the process.\n",
    "\n",
    "We are now ready to use USE! We will be returning back to embedding in more depth in Section 5, but as an example, we will embed some training and testing vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = sentence_df.text\n",
    "categories = sentence_df.sdg\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(docs, categories, test_size=0.33, random_state=7)\n",
    "\n",
    "X_train_use_vector = embed(X_train.tolist())\n",
    "X_test_use_vector = embed(X_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the embedding of the training vector looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(62202, 512), dtype=float32, numpy=\n",
       "array([[ 0.05109644,  0.05008151, -0.00049568, ..., -0.02072422,\n",
       "         0.03613124,  0.01162022],\n",
       "       [ 0.06358109, -0.0565166 , -0.05213946, ...,  0.00228676,\n",
       "        -0.04131475, -0.00125164],\n",
       "       [ 0.05327134, -0.02345247,  0.02675822, ...,  0.06473472,\n",
       "         0.0628962 ,  0.03613978],\n",
       "       ...,\n",
       "       [ 0.01227464,  0.00510351, -0.00631195, ...,  0.0456263 ,\n",
       "        -0.04639212, -0.07870013],\n",
       "       [ 0.06549167,  0.08835391,  0.00940975, ...,  0.03276522,\n",
       "         0.02704752, -0.00052535],\n",
       "       [-0.04562642, -0.00093942, -0.0363333 , ..., -0.03696235,\n",
       "        -0.03747366, -0.03414274]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_use_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is what the embedding of the testing vector looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30637, 512), dtype=float32, numpy=\n",
       "array([[-0.06285849,  0.05835171, -0.0444552 , ..., -0.02508012,\n",
       "        -0.04616681, -0.02821783],\n",
       "       [ 0.04709839, -0.03887603,  0.01155359, ...,  0.02760062,\n",
       "        -0.06390072, -0.0412748 ],\n",
       "       [ 0.01141804, -0.01593475, -0.04189894, ...,  0.05544081,\n",
       "         0.03996534,  0.03494091],\n",
       "       ...,\n",
       "       [ 0.07271196, -0.01527977, -0.02277797, ...,  0.02758262,\n",
       "        -0.00676865,  0.05427777],\n",
       "       [-0.07911933, -0.03813467,  0.00067008, ...,  0.00562062,\n",
       "         0.06976797, -0.041276  ],\n",
       "       [-0.04857083,  0.06084292, -0.03894068, ...,  0.01013256,\n",
       "        -0.03109165, -0.07248887]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_use_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Exercise 3.3\n",
    "Take two documents, one labeled as SDG 1 and the other as SDG 8. Segment these into sentences, compute the embedding, and find the dot product between the embeddings.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
