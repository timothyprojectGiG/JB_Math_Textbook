
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>14.3. About Text Data &#8212; An Introduction to Python Jupyter Notebooks for College Math Teachers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=07d05eac" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Advanced/AdvDataAnalysis/sec2_transform_features';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="14.4. Document Embedding" href="sec3_doc_embedding.html" />
    <link rel="prev" title="14.2. Preprocessing" href="sec1_preprocessing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="An Introduction to Python Jupyter Notebooks for College Math Teachers - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="An Introduction to Python Jupyter Notebooks for College Math Teachers - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    An Introduction to Python Jupyter Notebooks for College Math Teachers
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Preface.html">1. PREFACE</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../JMM23.html">1.1. JMM 2023</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../lab.html">1.2. JNB LAB: Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../labsolutions.html">1.3. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../PreCollege.html">2. PRE-COLLEGE - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/PreCollegeIntro.html">2.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Elementary/Arithmetic/elementary.html">2.2. Elementary Blackboard Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/GettingStarted/python.html">2.3. Beginning Python Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/Celestial/Chicago.html">2.4. Glimpse of Chicago</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/Celestial/steam.html">2.5. Arts in STEM (STEAM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/solutions.html">2.6. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/Celestial/demo.html">2.7. JNB LAB: After-School Program Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../PreCollege/Celestial/demosolutions.html">2.8. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Intropy.html">3. PYTHON PROGRAMMING GUIDE - Thomas VanDrunen, Yiheng Liang</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/PythonProgrammingGuideIntro.html">3.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/Introduction_to_Python.html">3.2. Introduction to Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/Data.html">3.3. Working with Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/solutions.html">3.4. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/Plab.html">3.5. JNB Lab: Introduction to Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Programming/Plabsolutions.html">3.6. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Data.html">4. EXPLORATORY DATA ANALYSIS - Jonathan Zhu</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec1_basic_intro.html">4.1. Basic EDA: Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec2_basic_tabular.html">4.2. Basic EDA: Data Investigation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec3_basic_plotting.html">4.3. Basic EDA: Plotting and Statistical Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec4_basic_solutions.html">4.4. Basic EDA: Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec5_advanced_intro.html">4.5. Advanced EDA: Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec6_advanced_tabular.html">4.6. Advanced EDA: Data Investigation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec7_advanced_plotting.html">4.7. Advanced EDA: Plotting and Statistical Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec8_advanced_report.html">4.8. Advanced EDA: Making EDA Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/sec9_advanced_solution.html">4.9. Advanced EDA: Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/lab.html">4.10. JNB Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/EDA/labsolution.html">4.11. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Prob.html">5. PROBABILITY - Laura Gross, Yiheng Liang</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob.html">5.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob1.html">5.2. Random Numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob2.html">5.3. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob3.html">5.4. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob4.html">5.5. Probability Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob4a.html">5.6. Random Walks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob5.html">5.7. Agent-based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/prob6.html">5.8. Mathematical Games</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/solutions.html">5.9. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/lab.html">5.10. JNB Lab Homicides in Chicago</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/ProbIntro/labsolution.html">5.11. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Stat.html">6. STATISTICAL INFERENCE - Peter Jantsch, Claire Wagner</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/StatInf/00_Instructor_Notes.html">6.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/StatInf/01_Foundations.html">6.2. Foundations of Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/StatInf/02_Inference_Categorical.html">6.3. Hypothesis Testing for Categorical Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/StatInf/03_Inference_Numerical.html">6.4. Hypothesis Testing for Numerical Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ProbStat/StatInf/04_Regression.html">6.5. Linear Regression</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../AppliedCalc.html">7. APPLIED CALCULUS FOR DAILY LIFE - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/AppliedCalculusIntro.html">7.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/twocommodity.html">7.2. Linear Systems and the Two-commodity Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/marginal.html">7.3. Marginal and Average Cost</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/design.html">7.4. Optimization and Object Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/cobbs.html">7.5. Optimization and Cobbs-Douglas Production</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/rates.html">7.6. Related Rates and Volumes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/football.html">7.7. Related Rates with Trig Functions and Football</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/jnb7.html">7.8. Probability Distributions and Drive Thrus</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/control.html">7.10. Normal Distribution and Process Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/ols.html">7.11. Partial Derivatives and OLS Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/gini.html">7.12. Area Between Curves and the Gini Index</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/income.html">7.13. Integral Test and Income Streams</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/ode.html">7.14. Ordinary Differential Equations and Exponential Growth/Decay</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/solutions.html">7.15. Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/AClab.html">7.16. JNB Lab: Calculus of Entropy in Daily Life and Monte Carlo Simulations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedCalc/AClabsolutions.html">7.17. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Calc.html">8. CALCULUS - Inne Singgih</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/Intro.html">8.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/2Functions.html">8.2. Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/3Limits.html">8.3. Limits, Continuity, and Rates</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/4%20Derivatives.html">8.4. Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/5%20Integrals.html">8.5. Integrals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/6%20ParametricEquations.html">8.6. Parametric Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/7%20Sequences%20and%20Series.html">8.7. Sequences and Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/Solutions.html">8.8. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/clab.html">8.9. JNB LAB: Calculus Animations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Calculus/clabsolution.html">8.10. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Linear.html">9. LINEAR ALGEBRA - Soheil Anbouhi</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/0.html">9.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/1.html">9.2. Linear Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/2.html">9.3. Matrices and Determinants</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/3.html">9.4. Linear Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/4.html">9.5. Eigenvalues and Eigenvectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/5.html">9.6. Orthogonality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/Solutions.html">9.7. Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/LAlab.html">9.8. JNB Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/Linear/LAlabsolutions.html">9.9. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Aplin.html">10. LINEAR ALGEBRA AND OPTIMIZATION FOR DATA ANALYSIS - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/Introduction.html">10.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/OLS/jnb1.html">10.2. OLS Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/KMeans/jnb2.html">10.3. K-means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/PCA/jnb3.html">10.4. Dimension Reduction by Principal Component Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/SVM/jnb4.html">10.5. Binary Classification of Labeled Data by Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/solutions.html">10.6. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/ALlab.html">10.7. JNB LAB: Linear Algebra for Data Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LinearAlgebra/ALlabsolution.html">10.8. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../DifEq.html">11. DIFFERENTIAL EQUATIONS - Rachel Petrik</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/Differential%20Equations.html">11.1. Overview of Chapter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DE2.html">11.2. Introduction to Differential Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DE3.html">11.3. First-Order Differential Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DE4.html">11.4. Second-Order Differential Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DE5.html">11.5. Systems of First-Order Differential Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DE6.html">11.6. Laplace Transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/DESolutions.html">11.7. Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/Differential%20Equations%20Book%201%20Lab.html">11.8. JNB Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/DifEq/Differential%20Equations%20Book%201%20Lab%20Answer%20Key.html">11.9. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../AppliedDifEq.html">12. DIFFERENTIAL EQUATIONS FOR THE BENEFIT OF SOCIETY - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/AppliedDiffEqIntro.html">12.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/covid.html">12.2. Logistic Growth and COVID-19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/sir.html">12.3. The Basic SIR Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/cholera.html">12.4. Cholera in Haiti</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/cws/cws.html">12.5. CWS Model of Alzheimer’s Disease</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/gravity/gravity.html">12.6. Gravity Fed Water Delivery</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/erc/erc.html">12.7. Earthquake Resistant Construction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/solutions.html">12.8. Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/hiv/lab.html">12.9. JNB Lab: HIV-AIDS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Undergrad/AppliedDifEq/labsolutions.html">12.10. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ComplexVar.html">13. COMPLEX VARIABLES IN GROUNDWATER MODELING - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_1.html">13.1. Setting the Scene</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_2.html">13.2. Complex Analysis Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_3.html">13.3. Idealized Groundwater Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_4.html">13.4. Contaminant Extraction Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_solutions.html">13.5. Solutions to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_lab.html">13.6. JNB LAB: Complex Potentials and Contaminant Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Complex/complex_variables_lab_solution.html">13.7. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../AdvData.html">14. ADVANCED DATA ANALYSIS - Ying Li, Jonathan Zhu, Claire Wagner</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="sec0_data.html">14.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec1_preprocessing.html">14.2. Preprocessing</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">14.3. About Text Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec3_doc_embedding.html">14.4. Document Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec4_classification_algos.html">14.5. Classification of Texts</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec5_classification_eval.html">14.6. Evaluating Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec6_applications.html">14.7. Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="solutions.html">14.8. Solutions to Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ComplexSystems.html">15. COMPLEX SYSTEMS - Wheaton College Team</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_1.html">15.1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_2.html">15.2. Fundamental Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_3.html">15.3. Mathematical Concepts from Equilibrium Statistical Physics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_4.html">15.4. Societal Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_solutions.html">15.5. Solution to Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_lab.html">15.6. JNB LAB: Complex Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComplexSystems/complex_systems_labsolution.html">15.7. JNB Lab Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/timothyprojectGiG/JB_Math_Textbook/main?urlpath=tree/src/Advanced/AdvDataAnalysis/sec2_transform_features.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/timothyprojectGiG/JB_Math_Textbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/timothyprojectGiG/JB_Math_Textbook/issues/new?title=Issue%20on%20page%20%2FAdvanced/AdvDataAnalysis/sec2_transform_features.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/Advanced/AdvDataAnalysis/sec2_transform_features.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>About Text Data</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformation">14.3.1. Transformation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizations">Vectorizations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-vectors">One-Hot Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-count-vectors">Numeric Count Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-term-matrices">Document-Term Matrices</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">N-Grams</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-language-modeling">Probabilistic Language Modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-generation">Natural Language Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#features">14.3.2. Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-term-frequency-inverse-document-frequency">TF-IDF (Term Frequency, Inverse Document Frequency)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-effectiveness">Feature Effectiveness</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-exercises">14.3.3. More Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_remove-input docutils container">
</div>
<section class="tex2jax_ignore mathjax_ignore" id="about-text-data">
<span id="index-0"></span><h1><span class="section-number">14.3. </span>About Text Data<a class="headerlink" href="#about-text-data" title="Link to this heading">#</a></h1>
<section id="transformation">
<h2><span class="section-number">14.3.1. </span>Transformation<a class="headerlink" href="#transformation" title="Link to this heading">#</a></h2>
<p>ML algorithms of any kind expect numerical features with fixed dimensions. This is especially the case for supervised learning algorithms such as classification algorithms, which is what we will be using to classify our texts into the UN SDG categories.</p>
<p>In documents, however, raw text data is typically semi-structured, with uneven lengths and varied contents. This poses a problem for ML methods, which typically operate on numerical vectors of equal length.</p>
<p>Therefore, once we have our preprocessed text data, our next job is to transform it, and the main way we do this is to <em>vectorize</em> it, or transform it into numerical vectors. When we map documents into vectors, we can use several techniques, which we will discuss later:</p>
<ul class="simple">
<li><p>bag of words</p></li>
<li><p>n-grams</p></li>
<li><p>word embedding</p></li>
</ul>
<p>Vectorization, and transformation of textual data as a whole, depends on tokenization. Carefully chosen preprocessing steps can help reduce complexity and increase ML model accuracy.</p>
<section id="vectorizations">
<span id="index-2"></span><span id="index-1"></span><h3>Vectorizations<a class="headerlink" href="#vectorizations" title="Link to this heading">#</a></h3>
<section id="bag-of-words">
<h4>Bag of Words<a class="headerlink" href="#bag-of-words" title="Link to this heading">#</a></h4>
<p>The <strong>bag of words</strong> approach treats a text or document almost literally as a bag of words; i.e., it ignores the order of the words and simply gathers all the words together in one big collection. All the words are then treated as features, with one feature for each word, where the value of the feature is the number of times that word appears in the document. We then set a fixed number of words to be considered, and our bag of words becomes the vocabulary we work with.</p>
<p>Let’s analyze the vocabulary of our UN SDG dataset. We use <code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> from the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library to create our bag of words, <em>fitting</em> it to the corpus to make it learn the vocabulary and <em>transforming</em> it into a matrix called <code class="docutils literal notranslate"><span class="pre">cv_fit</span></code> that gives the token counts for each term in each document. (This matrix is called a <strong>document-term matrix</strong>, a concept which will be explained more fully later on in this section.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">cv_fit</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p>Each row in <code class="docutils literal notranslate"><span class="pre">cv_fit</span></code> represents a bag of words for the corresponding document. Here are the first five rows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv_fit</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
</pre></div>
</div>
</div>
</div>
<p>We can use the function <code class="docutils literal notranslate"><span class="pre">get_feature_names_out()</span></code> to retrieve the learned vocabulary tokens, which have become the features of the bag of words. For example, the first feature (at index 0) is the token “00”, and the last feature (at index 45737) is the token “四个全面”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of features:&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">))</span>
<span class="n">feature_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of features: 45738
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;00&#39;, &#39;000&#39;, &#39;0000002&#39;, ..., &#39;œopen&#39;, &#39;ʿadawiyya&#39;, &#39;四个全面&#39;],
      dtype=object)
</pre></div>
</div>
</div>
</div>
<p>We can retrieve the total counts for the features by summing the counts in the matrix. We see, for example, that the token “00” occurs eight times in the entire corpus, while the token “四个全面” occurs twice.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_counts</span> <span class="o">=</span> <span class="n">cv_fit</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">total_counts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>matrix([[  8, 840,   1, ...,   1,   1,   2]])
</pre></div>
</div>
</div>
</div>
<p>To get a better idea of how the bag of words works, we can take a look at a short sample text, which is given below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_index</span> <span class="o">=</span> <span class="mi">724</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sample text:&#39;</span><span class="p">,</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sample_index</span><span class="p">])</span>
<span class="n">sample_bag</span> <span class="o">=</span> <span class="n">cv_fit</span><span class="p">[</span><span class="n">sample_index</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sample text: Peacemaking, peacekeeping, and peacebuilding may not have the punch and the means of national security, but they are receiving an increasing amount of attention in education, research, and politics.
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">inverse_transform()</span></code> function allows us to retrieve a list of all tokens from the vocabulary that occur at least once in the bag of words for this sample text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">sample_bag</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;the&#39;, &#39;of&#39;, &#39;have&#39;, &#39;in&#39;, &#39;and&#39;, &#39;are&#39;, &#39;but&#39;, &#39;education&#39;, &#39;an&#39;,
       &#39;research&#39;, &#39;security&#39;, &#39;they&#39;, &#39;may&#39;, &#39;not&#39;, &#39;national&#39;,
       &#39;politics&#39;, &#39;amount&#39;, &#39;increasing&#39;, &#39;attention&#39;, &#39;means&#39;,
       &#39;peacekeeping&#39;, &#39;receiving&#39;, &#39;peacemaking&#39;, &#39;peacebuilding&#39;,
       &#39;punch&#39;], dtype=&#39;&lt;U85&#39;)
</pre></div>
</div>
</div>
</div>
<p>The following code compares the counts of several of these tokens in the sample text to their total counts in the entire corpus. We use the attribute <code class="docutils literal notranslate"><span class="pre">vocabulary_</span></code>, which maps the feature names to their indices, to index into the counts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;peacemaking&#39;</span><span class="p">,</span> <span class="s1">&#39;punch&#39;</span><span class="p">,</span> <span class="s1">&#39;unknown&#39;</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">orient</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;sample count&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">sample_bag</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">token</span><span class="p">]]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">],</span>
    <span class="s1">&#39;total count&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">total_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">token</span><span class="p">]]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>and</th>
      <th>the</th>
      <th>peacemaking</th>
      <th>punch</th>
      <th>unknown</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>sample count</th>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>total count</th>
      <td>93357</td>
      <td>143100</td>
      <td>44</td>
      <td>1</td>
      <td>39</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Note how the tokens “and” and “the”, which are typically considered to be English stop words, occur more frequently than the other tokens. Meanwhile, the word “unknown” has a sample count of 0 because it does not occur in the sample text, even though it occurs 39 times in the rest of the corpus. We also discover that our sample text contains the only occurrence of the token “punch” in the entire corpus. (Words that occur only once in a corpus are known as <em>hapax legomena</em>.)</p>
<div class="admonition-exercise-2-1 admonition">
<p class="admonition-title">Exercise 2.1</p>
<p>What is the most common word in the corpus? (Hint: use <code class="docutils literal notranslate"><span class="pre">argmax()</span></code> on <code class="docutils literal notranslate"><span class="pre">total_counts</span></code> to find the index of the token.)</p>
</div>
<p>As we have seen, stop words like “and” and “the” are not removed by default. Fortunately, we can easily modify our code to remove stop words:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv2</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span> <span class="c1"># exclude English stop words</span>
<span class="n">cv2_fit</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_df</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">CountVectorizer()</span></code> provides a way to easily retrieve the stop words we are removing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">get_stop_words</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>frozenset({&#39;describe&#39;, &#39;whither&#39;, &#39;under&#39;, &#39;back&#39;, &#39;than&#39;, &#39;done&#39;, &#39;beside&#39;, &#39;first&#39;, &#39;up&#39;, &#39;being&#39;, &#39;moreover&#39;, &#39;he&#39;, &#39;detail&#39;, &#39;whereas&#39;, &#39;get&#39;, &#39;sometimes&#39;, &#39;herself&#39;, &#39;eleven&#39;, &#39;such&#39;, &#39;my&#39;, &#39;five&#39;, &#39;made&#39;, &#39;sixty&#39;, &#39;three&#39;, &#39;alone&#39;, &#39;any&#39;, &#39;thus&#39;, &#39;beforehand&#39;, &#39;over&#39;, &#39;which&#39;, &#39;everything&#39;, &#39;eight&#39;, &#39;only&#39;, &#39;move&#39;, &#39;others&#39;, &#39;therein&#39;, &#39;thereupon&#39;, &#39;amoungst&#39;, &#39;his&#39;, &#39;former&#39;, &#39;meanwhile&#39;, &#39;while&#39;, &#39;among&#39;, &#39;for&#39;, &#39;and&#39;, &#39;the&#39;, &#39;own&#39;, &#39;no&#39;, &#39;whence&#39;, &#39;per&#39;, &#39;be&#39;, &#39;whether&#39;, &#39;too&#39;, &#39;ten&#39;, &#39;behind&#39;, &#39;whole&#39;, &#39;she&#39;, &#39;towards&#39;, &#39;along&#39;, &#39;was&#39;, &#39;must&#39;, &#39;but&#39;, &#39;once&#39;, &#39;re&#39;, &#39;with&#39;, &#39;who&#39;, &#39;by&#39;, &#39;us&#39;, &#39;except&#39;, &#39;some&#39;, &#39;it&#39;, &#39;have&#39;, &#39;ie&#39;, &#39;ours&#39;, &#39;front&#39;, &#39;whose&#39;, &#39;several&#39;, &#39;indeed&#39;, &#39;rather&#39;, &#39;bill&#39;, &#39;many&#39;, &#39;further&#39;, &#39;full&#39;, &#39;please&#39;, &#39;formerly&#39;, &#39;into&#39;, &#39;because&#39;, &#39;anywhere&#39;, &#39;latter&#39;, &#39;via&#39;, &#39;con&#39;, &#39;everywhere&#39;, &#39;etc&#39;, &#39;fire&#39;, &#39;her&#39;, &#39;you&#39;, &#39;becomes&#39;, &#39;whenever&#39;, &#39;both&#39;, &#39;whereupon&#39;, &#39;beyond&#39;, &#39;from&#39;, &#39;would&#39;, &#39;here&#39;, &#39;thick&#39;, &#39;anyway&#39;, &#39;in&#39;, &#39;nowhere&#39;, &#39;other&#39;, &#39;where&#39;, &#39;becoming&#39;, &#39;nothing&#39;, &#39;seeming&#39;, &#39;when&#39;, &#39;somewhere&#39;, &#39;could&#39;, &#39;nine&#39;, &#39;put&#39;, &#39;now&#39;, &#39;cannot&#39;, &#39;also&#39;, &#39;there&#39;, &#39;nor&#39;, &#39;without&#39;, &#39;below&#39;, &#39;that&#39;, &#39;thin&#39;, &#39;become&#39;, &#39;seem&#39;, &#39;these&#39;, &#39;fifty&#39;, &#39;they&#39;, &#39;were&#39;, &#39;something&#39;, &#39;can&#39;, &#39;almost&#39;, &#39;anything&#39;, &#39;never&#39;, &#39;show&#39;, &#39;had&#39;, &#39;most&#39;, &#39;inc&#39;, &#39;enough&#39;, &#39;so&#39;, &#39;bottom&#39;, &#39;therefore&#39;, &#39;six&#39;, &#39;hereby&#39;, &#39;four&#39;, &#39;herein&#39;, &#39;much&#39;, &#39;yourself&#39;, &#39;whereby&#39;, &#39;toward&#39;, &#39;has&#39;, &#39;mill&#39;, &#39;ourselves&#39;, &#39;de&#39;, &#39;this&#39;, &#39;serious&#39;, &#39;across&#39;, &#39;system&#39;, &#39;couldnt&#39;, &#39;what&#39;, &#39;always&#39;, &#39;take&#39;, &#39;nevertheless&#39;, &#39;last&#39;, &#39;very&#39;, &#39;seems&#39;, &#39;twenty&#39;, &#39;against&#39;, &#39;whoever&#39;, &#39;am&#39;, &#39;less&#39;, &#39;fifteen&#39;, &#39;hers&#39;, &#39;out&#39;, &#39;after&#39;, &#39;afterwards&#39;, &#39;down&#39;, &#39;few&#39;, &#39;well&#39;, &#39;thru&#39;, &#39;not&#39;, &#39;sincere&#39;, &#39;mine&#39;, &#39;me&#39;, &#39;between&#39;, &#39;do&#39;, &#39;interest&#39;, &#39;third&#39;, &#39;ever&#39;, &#39;each&#39;, &#39;side&#39;, &#39;before&#39;, &#39;an&#39;, &#39;during&#39;, &#39;anyhow&#39;, &#39;around&#39;, &#39;as&#39;, &#39;forty&#39;, &#39;hereupon&#39;, &#39;hence&#39;, &#39;another&#39;, &#39;two&#39;, &#39;eg&#39;, &#39;at&#39;, &#39;may&#39;, &#39;hundred&#39;, &#39;of&#39;, &#39;one&#39;, &#39;every&#39;, &#39;those&#39;, &#39;are&#39;, &#39;hereafter&#39;, &#39;find&#39;, &#39;someone&#39;, &#39;sometime&#39;, &#39;to&#39;, &#39;found&#39;, &#39;why&#39;, &#39;though&#39;, &#39;him&#39;, &#39;otherwise&#39;, &#39;yours&#39;, &#39;keep&#39;, &#39;our&#39;, &#39;will&#39;, &#39;still&#39;, &#39;wherever&#39;, &#39;co&#39;, &#39;latterly&#39;, &#39;onto&#39;, &#39;everyone&#39;, &#39;themselves&#39;, &#39;about&#39;, &#39;throughout&#39;, &#39;became&#39;, &#39;through&#39;, &#39;although&#39;, &#39;cant&#39;, &#39;within&#39;, &#39;might&#39;, &#39;yet&#39;, &#39;perhaps&#39;, &#39;top&#39;, &#39;hasnt&#39;, &#39;whom&#39;, &#39;call&#39;, &#39;already&#39;, &#39;part&#39;, &#39;however&#39;, &#39;again&#39;, &#39;twelve&#39;, &#39;thereafter&#39;, &#39;either&#39;, &#39;wherein&#39;, &#39;thereby&#39;, &#39;else&#39;, &#39;them&#39;, &#39;thence&#39;, &#39;how&#39;, &#39;if&#39;, &#39;upon&#39;, &#39;its&#39;, &#39;since&#39;, &#39;whereafter&#39;, &#39;amongst&#39;, &#39;empty&#39;, &#39;mostly&#39;, &#39;name&#39;, &#39;together&#39;, &#39;i&#39;, &#39;should&#39;, &#39;we&#39;, &#39;or&#39;, &#39;is&#39;, &#39;fill&#39;, &#39;neither&#39;, &#39;noone&#39;, &#39;until&#39;, &#39;cry&#39;, &#39;besides&#39;, &#39;itself&#39;, &#39;somehow&#39;, &#39;whatever&#39;, &#39;all&#39;, &#39;due&#39;, &#39;their&#39;, &#39;a&#39;, &#39;least&#39;, &#39;next&#39;, &#39;your&#39;, &#39;elsewhere&#39;, &#39;often&#39;, &#39;then&#39;, &#39;been&#39;, &#39;see&#39;, &#39;seemed&#39;, &#39;un&#39;, &#39;more&#39;, &#39;go&#39;, &#39;yourselves&#39;, &#39;even&#39;, &#39;off&#39;, &#39;himself&#39;, &#39;myself&#39;, &#39;amount&#39;, &#39;give&#39;, &#39;none&#39;, &#39;same&#39;, &#39;ltd&#39;, &#39;above&#39;, &#39;namely&#39;, &#39;anyone&#39;, &#39;on&#39;, &#39;nobody&#39;})
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise-2-2 admonition">
<p class="admonition-title">Exercise 2.2</p>
<p>Find the length of the vocabulary (the number of features) without stop words, and also find the length of the list of stop words. Is the difference between our original vocabulary and our vocabulary with stop words removed equal to the number of stop words? Why or why not?</p>
</div>
<div class="admonition-exercise-2-3 admonition">
<p class="admonition-title">Exercise 2.3</p>
<p>What is the most common word now that stop words have been removed?</p>
</div>
<p>Now that stop words have been excluded from the vocabulary, the list of vocabulary tokens in our sample text with non-zero counts has changed as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_bag</span> <span class="o">=</span> <span class="n">cv2_fit</span><span class="p">[</span><span class="n">sample_index</span><span class="p">]</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">sample_bag</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;education&#39;, &#39;research&#39;, &#39;security&#39;, &#39;national&#39;, &#39;politics&#39;,
       &#39;increasing&#39;, &#39;attention&#39;, &#39;means&#39;, &#39;peacekeeping&#39;, &#39;receiving&#39;,
       &#39;peacemaking&#39;, &#39;peacebuilding&#39;, &#39;punch&#39;], dtype=&#39;&lt;U85&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-hot-vectors">
<span id="index-5"></span><span id="index-4"></span><span id="index-3"></span><h4>One-Hot Vectors<a class="headerlink" href="#one-hot-vectors" title="Link to this heading">#</a></h4>
<p>Another vectorization approach is to treat a text as a <strong>one-hot vector</strong>. This approach lines up all words in the vocabulary and assigns each of them an index <span class="math notranslate nohighlight">\(1 \le i \le V\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the number of words in the vocabulary. For each word, it then forms a vector</p>
<p><span class="math notranslate nohighlight">\(\overrightarrow{w} = (0, 0, …, 0, 1, 0, 0, …, 0)\)</span> ,</p>
<p>with only one 1 in the vector and the rest of the numbers being zeroes. The 1 is at the position <span class="math notranslate nohighlight">\(i\)</span> that indexes the word in our vocabulary. The order of the words is ignored, so it does not matter if a certain word is indexed before another one. Because all of the words in our vocabulary are unique, there are no similarity notions among the vectors.</p>
<p>To gain a better understanding of how one-hot vectors work, we can take a look at how a short sample text from the UN SDG dataset would be encoded. As before, we use <code class="docutils literal notranslate"><span class="pre">NLTK</span></code> to tokenize the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_text</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">12737</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sample text:&#39;</span><span class="p">,</span> <span class="n">sample_text</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sample_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tokens:&#39;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sample text: the chapter examines the contribution that international relations theory has made to the reading and practice of peacebuilding.
Tokens: [&#39;the&#39;, &#39;chapter&#39;, &#39;examines&#39;, &#39;the&#39;, &#39;contribution&#39;, &#39;that&#39;, &#39;international&#39;, &#39;relations&#39;, &#39;theory&#39;, &#39;has&#39;, &#39;made&#39;, &#39;to&#39;, &#39;the&#39;, &#39;reading&#39;, &#39;and&#39;, &#39;practice&#39;, &#39;of&#39;, &#39;peacebuilding&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<p>We also define a convenience function to reshape the list of tokens into a format that can be used by the one-hot encoder:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ohe_reshape</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">OneHotEncoder()</span></code> from the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> library to create the one-hot encoding. We fit the encoder to our sample text to make it learn the vocabulary. As with <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>, we can view the resulting features using the function <code class="docutils literal notranslate"><span class="pre">get_feature_names_out()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ohe</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span> <span class="c1"># encode unknown tokens as vectors of all zeros</span>
<span class="n">ohe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ohe_reshape</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="n">ohe</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;x0_.&#39;, &#39;x0_and&#39;, &#39;x0_chapter&#39;, &#39;x0_contribution&#39;, &#39;x0_examines&#39;,
       &#39;x0_has&#39;, &#39;x0_international&#39;, &#39;x0_made&#39;, &#39;x0_of&#39;,
       &#39;x0_peacebuilding&#39;, &#39;x0_practice&#39;, &#39;x0_reading&#39;, &#39;x0_relations&#39;,
       &#39;x0_that&#39;, &#39;x0_the&#39;, &#39;x0_theory&#39;, &#39;x0_to&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>Now that the encoder has learned the vocabulary, we can try it out on text. The following code transforms the first five tokens of our sample text into one-hot encodings by calling the <code class="docutils literal notranslate"><span class="pre">transform</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;First five tokens:&#39;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="n">ohe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ohe_reshape</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">5</span><span class="p">]))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First five tokens: [&#39;the&#39;, &#39;chapter&#39;, &#39;examines&#39;, &#39;the&#39;, &#39;contribution&#39;]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.],
       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.],
       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.],
       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.]])
</pre></div>
</div>
</div>
</div>
<p>Note how, as explained above, each unique token in the learned vocabulary has a unique encoding, with only one 1 in the vector. Note also how the first and fourth vectors in this matrix are identical because they represent the same token, “the”.</p>
<p>We can view the unique encoding of “the” by using the <code class="docutils literal notranslate"><span class="pre">transform</span></code> function again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ohe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ohe_reshape</span><span class="p">([</span><span class="s1">&#39;the&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])
</pre></div>
</div>
</div>
</div>
<p>If we try to encode a token that isn’t included in the learned vocabulary, however, we get a vector of only zeros:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ohe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ohe_reshape</span><span class="p">([</span><span class="s1">&#39;unknown&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">toarray</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise-2-4 admonition">
<p class="admonition-title">Exercise 2.4</p>
<p>Verify that each token in the learned vocabulary has a unique encoding.</p>
</div>
</section>
<section id="numeric-count-vectors">
<h4>Numeric Count Vectors<a class="headerlink" href="#numeric-count-vectors" title="Link to this heading">#</a></h4>
<p>A third vectorization approach is to make a numeric vector of counts. This approach counts the occurence of the words in the document and lines up the counts of words in a row for each document. Numeric count vectors are similar to document-term matrices, although they differ in that they measure words rather than tokens (which can be words, but can also be larger units such as phrases or smaller units such as affixes). As a result, we will focus on document-term matrices, which are covered in the next part of this section.</p>
</section>
<section id="document-term-matrices">
<h4>Document-Term Matrices<a class="headerlink" href="#document-term-matrices" title="Link to this heading">#</a></h4>
<p>One final vectorization approach we will introduce here is to form a document-term matrix. To form this matrix, we define the matrix’s columns to be tokens (such as words or phrases), the matrix’s rows to be the documents in the collection, and the value of each entry to be the frequency of each token occuring in the document. Additionally, the values in each entry are weighted frequencies. One weighting method we use is <strong>TF-IDF</strong>, or term frequency (TF) times inverse document frequency (IDF). TF refers to the frequency of the word in the document, while IDF refers to the inverse of the number of documents containing the word divided by number of documents. TF-IDF is discussed later in this section.</p>
<p>Below, we compute a simple document-term matrix for a portion of the documents and tokens in the UN SDG dataset. The features are all unigrams (one-word tokens). While the unigram does provide context for each word, NLP tasks built on such simple model would be disadvantaged.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span>
<span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">count_vector</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">count_vector_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count_vector</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="n">term_freq</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;term&quot;</span><span class="p">:</span> <span class="n">count_vector_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="s2">&quot;freq&quot;</span> <span class="p">:</span> <span class="n">count_vector_df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)})</span>
<span class="n">count_vector_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">110</span><span class="p">:</span><span class="mi">120</span><span class="p">,</span><span class="n">term_freq</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;freq&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">term</span><span class="p">]</span> <span class="c1"># take a portion</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>countries</th>
      <th>women</th>
      <th>development</th>
      <th>health</th>
      <th>water</th>
      <th>public</th>
      <th>social</th>
      <th>education</th>
      <th>policy</th>
      <th>international</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>110</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>117</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0</td>
      <td>4</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="admonition-exercise-2-5 admonition">
<p class="admonition-title">Exercise 2.5</p>
<p>Look at document 118.</p>
<ul class="simple">
<li><p>What are the most frequent words for this document?</p></li>
<li><p>Refer to the UN SDGs. What is your best guess as to which goal this document describes?</p></li>
</ul>
</div>
</section>
</section>
<section id="n-grams">
<span id="index-6"></span><h3>N-Grams<a class="headerlink" href="#n-grams" title="Link to this heading">#</a></h3>
<p>An n-gram is a contiguous sequence of <span class="math notranslate nohighlight">\(n\)</span> items from a text. Items in an n-gram are tokens we choose to include through preprocessing, but these items can also include markers we add to the data to serve the NLP tasks at hand.</p>
<p>For example, if we were to find what words are often used at the beginning or end of a sentence, we would add &lt;s&gt; and &lt;/s&gt; as markers for the beginnings and ends of sentences, to denote the sentence boundaries. We would then build bigrams <span class="math notranslate nohighlight">\((n=2)\)</span>; for the sentence “I built an AI machine”, we would get the items
(&lt;s&gt; I), (I built), (built an), (an AI), (AI machine), and (machine &lt;/s&gt;). We then count the frequency of bigrams. specifically of the form (&lt;s&gt; *) and (* &lt;/s&gt;), where * represents any other token.</p>
<p>In the previous subsection, we looked at a document-term matrix of unigrams (with stop words removed). We can check the total count of unigrams — as well as the number of <em>unique</em> unigrams — with the following code. Notice how the second number is smaller than the first because many of the unigrams occur more than once in the corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total count of unigrams (without stop words):&#39;</span><span class="p">,</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of unique unigrams (without stop words):&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total count of unigrams (without stop words): 1351051
Number of unique unigrams (without stop words): 45440
</pre></div>
</div>
</div>
</div>
<p>We then compare this to the counts obtained when stop words are not removed, which we calculate using the <code class="docutils literal notranslate"><span class="pre">cv</span></code> and <code class="docutils literal notranslate"><span class="pre">cv_fit</span></code> objects from earlier in this section:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total count of unigrams (with stop words):&#39;</span><span class="p">,</span> <span class="n">cv_fit</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of unique unigrams (with stop words):&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total count of unigrams (with stop words): 2335467
Number of unique unigrams (with stop words): 45738
</pre></div>
</div>
</div>
</div>
<p>Notice that stop word removal alone reduced the total count of terms by around 42%, even though it decreased the number of unique terms in the vocabulary only slightly.</p>
<p>Next, we can check the bigrams for our document corpus by setting the <code class="docutils literal notranslate"><span class="pre">ngram_range</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span> 
<span class="n">count_vector</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total count of bigrams (without stop words):&#39;</span><span class="p">,</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of unique bigrams (without stop words):&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total count of bigrams (without stop words): 1326382
Number of unique bigrams (without stop words): 834465
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise-2-6 admonition">
<p class="admonition-title">Exercise 2.6</p>
<p>Notice that the number of unique bigrams is larger than the number of unique unigrams, even though the total count of bigrams is smaller than the total count of unigrams. Why might this be the case?</p>
</div>
<div class="admonition-exercise-2-7 admonition">
<p class="admonition-title">Exercise 2.7</p>
<p>Modify the above code to get the counts of the trigrams without stop words. Is the total count of trigrams larger or smaller than the total count of bigrams? What about the number of unique trigrams compared to the number of unique bigrams? Why might this be the case?</p>
</div>
</section>
<section id="probabilistic-language-modeling">
<span id="index-8"></span><span id="index-7"></span><h3>Probabilistic Language Modeling<a class="headerlink" href="#probabilistic-language-modeling" title="Link to this heading">#</a></h3>
<p>So far, we have worked with clearly defined texts and sequences of words. But sometimes, the texts we get are incomplete, or we might want to reduce computational time by only giving portions of the text. As such, we might turn to probabilistic language modeling. Typically, a probabilistic language model analyzes a body of text data and computes the following based on that body of text data:</p>
<ul class="simple">
<li><p>the probability of a sentence or sequence of words occurring, <span class="math notranslate nohighlight">\(P(w_1, w_2, w_3, … , w_n)\)</span>,</p></li>
<li><p>the probability of the next word given <span class="math notranslate nohighlight">\(k\)</span> words, <span class="math notranslate nohighlight">\(P(w_n | w_{(n-k)}, w_{(n-(k-1))}, … , w_{(n-1)})\)</span>.</p></li>
</ul>
<p>We then use the chain rule to compute the joint probability:</p>
<p><span class="math notranslate nohighlight">\(P(A \cap B) = P(B|A)*P(A)\)</span></p>
<p><span class="math notranslate nohighlight">\(\implies P(w_1, w_2, w_3, …, w_n) = 
P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_n|w_1, …, w_{(n-1)})\)</span>.</p>
<p>We could estimate the probability by calculating</p>
<p><span class="math notranslate nohighlight">\(\text{count}(w_1, w_2, …, w_{(i-1)}, w_i) / \text{count}(w_1, w_2, …, w_{(i-1)})\)</span>,</p>
<p>but with too many possible occurrences, this calculation is not feasible.</p>
<p>Instead, we can simplify the equation through the Markov Assumption, which in an NLP context assumes that the next word only depends on some previous words, not all of them. This means we can place a condition on the previous <span class="math notranslate nohighlight">\(k\)</span> words rather than all the previous words:</p>
<p><span class="math notranslate nohighlight">\(P(w_n|w_1, w_2, w_3, …, w_{(n-1)}) \sim  P(w_n|w_{(n-k)}, …, w_{(n-1)})\)</span>.</p>
<p>When considering a unigram <span class="math notranslate nohighlight">\((n=1)\)</span> model, we calculate the probability as</p>
<p><span class="math notranslate nohighlight">\(P(w_1, w_2, w_3, …, w_n) = P(w_1)P(w_2)P(w_3)...P(w_n)\)</span>.</p>
<p>A similar process occurs when considering a bigram <span class="math notranslate nohighlight">\((n=2)\)</span> model, where we place a condition on the previous word,</p>
<p><span class="math notranslate nohighlight">\(P(w_n|w_1, …, w_{(n-1)}) \sim P(w_i|w_{(i-1)})\)</span>,</p>
<p>and we can similarly extend this process to trigrams, 4-grams, 5-grams, or any n-gram.</p>
<p>However, no matter how many <span class="math notranslate nohighlight">\(n\)</span> we consider, we still will not arrive at a sufficient model; language has long-distance dependencies that require extremely large <span class="math notranslate nohighlight">\(n\)</span> that can even extend to previous sentences and/or documents. To truly determine if our n-gram is sufficient, we extrinsically evaluate it by putting our model to tasks. For example, if we only want to deal with spell correction, a unigram be sufficient, but for machine translation, we might need to work with very long sentences. Google’s recent speech model features 20B sentences (<a class="reference external" href="https://arxiv.org/abs/2303.01037?utm_source=substack&amp;amp;utm_medium=email">https://arxiv.org/abs/2303.01037?utm_source=substack&amp;utm_medium=email</a>).</p>
</section>
<section id="natural-language-generation">
<span id="index-9"></span><h3>Natural Language Generation<a class="headerlink" href="#natural-language-generation" title="Link to this heading">#</a></h3>
<p>Natural Language Generation (NLG) is an offshoot of Probabilistic Language Modeling which is used to generate texts. Given a prompt (such as a short incomplete sentence), NLG models will generate text in response (such as giving the next few words of the sentence).</p>
<div class="admonition-exercise-2-8 admonition">
<p class="admonition-title">Exercise 2.8</p>
<p>Visit this Hugging Face demo, which attempts to predict the next token when given part of a sentence: <a class="reference external" href="https://alonsosilva-nexttokenprediction.hf.space/">https://alonsosilva-nexttokenprediction.hf.space/</a>. Try typing in a few incomplete sentences. How accurate are the model’s predictions?</p>
</div>
<p>As the exercise shows, these language models are not perfect. But especially with recent developments in AI, they are improving.</p>
<p>Another main role of NLG is automatic summarization of a text. The NLG has the ability to perform extractive summarization (extracting meaning from a text), or abstractive summarization (constructing an abstract from a text). NLG can also be used for topic modeling to identify the main topics of the text, or as an advanced machine translation tool.</p>
<p>An example of an advanced natural language generator is the Generative Pre-trained Transformer (GPT), mainly pioneered by the startup OpenAI. The GPT is a text generation deep learning model trained on massive datasets, including internet data, book data, GitHub data, and more. GPTs, including the famous ChatGPT, can take thousands of words as input and have been trained on billions of different parameters.</p>
</section>
</section>
<section id="features">
<span id="index-11"></span><span id="index-10"></span><h2><span class="section-number">14.3.2. </span>Features<a class="headerlink" href="#features" title="Link to this heading">#</a></h2>
<section id="tf-idf-term-frequency-inverse-document-frequency">
<h3>TF-IDF (Term Frequency, Inverse Document Frequency)<a class="headerlink" href="#tf-idf-term-frequency-inverse-document-frequency" title="Link to this heading">#</a></h3>
<p>In the document-term matrix previously discussed, we represented columns as tokens, rows as documents, and the entry values as the number of occurrences of that token in the document. However, using the counts alone may not give us an accurate picture of how significant each token is in context, since some tokens naturally occur more frequently than others. We can ameliorate this problem by weighting the frequencies with TF-IDF.</p>
<p>In our computation of the matrix cell <span class="math notranslate nohighlight">\((d,t)\)</span> with document as row and term as column, we compute the following:</p>
<ul class="simple">
<li><p>TF: term frequency of term <span class="math notranslate nohighlight">\(t\)</span> in the document <span class="math notranslate nohighlight">\(d\)</span> =
count of term <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span> divided by total number of terms in document <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p>DF: document frequency of term <span class="math notranslate nohighlight">\(t\)</span> =
number of documents containing term <span class="math notranslate nohighlight">\(t\)</span> divided by total number of documents
<span class="math notranslate nohighlight">\(\frac{\text{df}}{N}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the total number of documents and is a constant that can be ignored</p></li>
<li><p>IDF: take the inverse of DF</p></li>
<li><p>TF-IDF = TF * IDF</p></li>
</ul>
<p>We can use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to get TF-IDF with <code class="docutils literal notranslate"><span class="pre">scikit-learn.feature_extraction.text.TfidfVectorizer</span></code>. This code takes several arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sublinear_df</span></code> : Boolean; if true, uses a logarithmic form for frequency,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_df</span></code> : the minimum number of documents a word must be present in to be kept,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">norm</span></code>: usually set to l2 to ensure all our feature vectors have a Euclidean norm of 1,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ngram_range</span></code>: determines whether to take unigrams, bigrams, or both, and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stop_words</span></code>:  usually set to “english” for removing English stop words.</p></li>
</ul>
<p>Note that when using logarithmic form for term frequency, the weighted TF is transformed into <span class="math notranslate nohighlight">\(1 + \log \text{TF}\)</span>; if <span class="math notranslate nohighlight">\(\text{TF} &gt; 0\)</span>, then <span class="math notranslate nohighlight">\(\text{TF} = 0\)</span>; otherwise, we use <span class="math notranslate nohighlight">\(\log\frac{N}{\text{df}}\)</span> for IDF to dampen the effect of <span class="math notranslate nohighlight">\(\frac{N}{\text{df}}\)</span>.</p>
<p>As an example, we compute the same document-term matrix as before, but this time using <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code>. We include only unigrams and remove English stop words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span>
<span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">tfidf_vector</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">tfidf_vector_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">tfidf_vector</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="n">term_freq</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;term&quot;</span><span class="p">:</span> <span class="n">tfidf_vector_df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="s2">&quot;freq&quot;</span> <span class="p">:</span> <span class="n">tfidf_vector_df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)})</span>
<span class="n">tfidf_vector_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">110</span><span class="p">:</span><span class="mi">120</span><span class="p">,</span><span class="n">term_freq</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;freq&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">term</span><span class="p">]</span> <span class="c1"># take a portion</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>countries</th>
      <th>women</th>
      <th>water</th>
      <th>health</th>
      <th>development</th>
      <th>education</th>
      <th>energy</th>
      <th>social</th>
      <th>public</th>
      <th>policy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>110</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.144528</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.055794</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.150946</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.073888</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.105940</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.060643</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.128756</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.06901</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.134099</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.082325</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.000000</td>
      <td>0.244198</td>
      <td>0.0</td>
      <td>0.063116</td>
      <td>0.216640</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.059071</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.000000</td>
      <td>0.097945</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.046368</td>
      <td>0.00000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="feature-effectiveness">
<span id="index-16"></span><span id="index-15"></span><span id="index-14"></span><span id="index-13"></span><span id="index-12"></span><h3>Feature Effectiveness<a class="headerlink" href="#feature-effectiveness" title="Link to this heading">#</a></h3>
<p>In examining how to evaluate the effectiveness of features in the context of NLP tasks, we will mainly use the task of document classification and classifying our texts into SDG categories as mentioned in the preface of this section.</p>
<p>When looking at document classification, we can use a confusion matrix and get <span class="math notranslate nohighlight">\(tp, fn, fp, tn\)</span>; these can be used to calculate the precision, recall, and f1 measures as follows:</p>
<ul class="simple">
<li><p>precision: <span class="math notranslate nohighlight">\(\frac{tp}{tp+fp}\)</span>,</p></li>
<li><p>recall: <span class="math notranslate nohighlight">\(\frac{tp}{tp+fn}\)</span>,</p></li>
<li><p>f1: <span class="math notranslate nohighlight">\(2 * \frac{\text{precision } * \text{ recall}}{\text{precision } + \text{ recall}}\)</span>.</p></li>
</ul>
<p>Below, we use a multinomial Naive Bayes algorithm to classify our documents by their SDG; the process of this is explained in later sections. For now, we examine the multiclass confusion matrix classifying documents by their respective UN SDG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span>
<span class="n">categories</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">sdg</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="n">X_train_count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span> <span class="o">=</span> <span class="s2">&quot;english&quot;</span> <span class="p">)</span>
<span class="n">X_train_count_vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> 
<span class="n">X_train_count_vector</span> <span class="o">=</span> <span class="n">X_train_count_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> 
<span class="n">X_test_count_vector</span> <span class="o">=</span> <span class="n">X_train_count_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> 

<span class="n">count_multinomialNB_clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_count_vector</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">count_multinomialNB_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_count_vector</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="s1">&#39;heavy&#39;</span><span class="p">,</span><span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,}</span>
<span class="n">ConfusionMatrixDisplay</span><span class="o">.</span><span class="n">from_predictions</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">text_kw</span><span class="o">=</span><span class="n">font</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">colormaps</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/37567cf1840138daf366e2e4a85e18d2361e5c3d423595e472aee1d0e70a5b4a.png" src="../../_images/37567cf1840138daf366e2e4a85e18d2361e5c3d423595e472aee1d0e70a5b4a.png" />
</div>
</div>
<p>When looking at the metrics for SDG 1, we find that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(tp = 398\)</span> (where predicted label and true label are the same),</p></li>
<li><p><span class="math notranslate nohighlight">\(fp = 153\)</span> (calculated by summing vertically below 398),</p></li>
<li><p><span class="math notranslate nohighlight">\(fn = 83\)</span> (calculated by summing horizontally to the right of 398).</p></li>
</ul>
<div class="admonition-exercise-2-9 admonition">
<p class="admonition-title">Exercise 2.9</p>
<p>Use the above values and formulas to calculate precision, recall, and f1 for SDG 1.</p>
</div>
</section>
</section>
<section id="more-exercises">
<h2><span class="section-number">14.3.3. </span>More Exercises<a class="headerlink" href="#more-exercises" title="Link to this heading">#</a></h2>
<div class="admonition-exercise-2-10 admonition">
<p class="admonition-title">Exercise 2.10</p>
<p>Using the UN SDG dataset, begin working with some of the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> NLP capabilities, such as the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="n">text_df</span><span class="o">.</span><span class="n">text</span>
<span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count_vector</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">count_vector_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count_vector</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
<p>Try out different parameters in <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>, including</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ngram_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span>
<span class="n">min_df</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">min_df</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
<p>What are the most frequent unigrams, bigrams, and trigrams? Answer this question with, and then without, stop word removal.</p>
</div>
<div class="admonition-exercise-2-11 admonition">
<p class="admonition-title">Exercise 2.11</p>
<p>Write a function that takes in a document corpus like the UN SDG dataset and returns:</p>
<ul class="simple">
<li><p>The top 50 most frequent words,</p></li>
<li><p>A plot of the cumulative word count from the most frequent word to the 50th most frequent, and</p></li>
<li><p>A comparison of the level of cumulation (i.e., the height where the plot ends) with the total number of words of the input corpus, outputting the percentage.</p></li>
</ul>
<p>Your function should also contain a parameter <code class="docutils literal notranslate"><span class="pre">stop_words</span></code> with its default set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, which does not remove stop words.</p>
</div>
<div class="admonition-exercise-2-12 admonition">
<p class="admonition-title">Exercise 2.12</p>
<p>Run your function from Exercise 2.11 on the documents labeled with SDG 8. What is the output?</p>
</div>
<div class="admonition-exercise-2-13 admonition">
<p class="admonition-title">Exercise 2.13</p>
<p>Run your function from Exercise 2.11 on the entire UN SDG corpus, once with stop removal and once without. Describe the similarities and differences in the outputs when this parameter is changed.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Advanced/AdvDataAnalysis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="sec1_preprocessing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">14.2. </span>Preprocessing</p>
      </div>
    </a>
    <a class="right-next"
       href="sec3_doc_embedding.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">14.4. </span>Document Embedding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformation">14.3.1. Transformation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizations">Vectorizations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-vectors">One-Hot Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numeric-count-vectors">Numeric Count Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-term-matrices">Document-Term Matrices</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">N-Grams</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-language-modeling">Probabilistic Language Modeling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-generation">Natural Language Generation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#features">14.3.2. Features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf-term-frequency-inverse-document-frequency">TF-IDF (Term Frequency, Inverse Document Frequency)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-effectiveness">Feature Effectiveness</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-exercises">14.3.3. More Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Paul Isihara, Claire Wagner, Peter Jantsch, and Thomas VanDrunen, Editors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>
  <a rel='license' href='http://creativecommons.org/licenses/by/4.0/'><img alt='Creative Commons License' width=88 style='border-width:0' src='https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png'/></a>
  Except where otherwise noted, this work is licensed under a <a rel='license' href='http://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 International License</a>.
</p>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>